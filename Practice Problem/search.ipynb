{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read each text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of .txt files:\n",
      "01.txt.txt\n",
      "02.txt.txt\n",
      "03.txt.txt\n",
      "04.txt.txt\n",
      "05.txt.txt\n",
      "06.txt.txt\n",
      "07.txt.txt\n",
      "08.txt.txt\n",
      "09.txt.txt\n",
      "10.txt.txt\n",
      "11.txt.txt\n",
      "12.txt.txt\n",
      "13.txt.txt\n",
      "14.txt.txt\n",
      "15.txt.txt\n",
      "16.txt.txt\n",
      "17.txt.txt\n",
      "18.txt.txt\n",
      "19.txt.txt\n",
      "20.txt.txt\n"
     ]
    }
   ],
   "source": [
    "import glob # glob is a module that we used to find all files that match specified pattern\n",
    "import os\n",
    "folder_path = '/Volumes/Jagannath/Fall 2024/Cloud Computing/Assignment_1_data/Assignment1 txt files'\n",
    "file_names = sorted([os.path.basename(f) for f in glob.glob(os.path.join(folder_path, '*.txt'))])\n",
    "print(\"List of .txt files:\", *file_names, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After a 15-month suspension for drug use, Maria Sharapova returned to the U.S. Open for the first time in three years on Monday night. The 30-year-old put on a memorable performance under the lights of Arthur Ashe Stadium, downing No. 2 seed Simona Halep. Today, she'll face Timea Babos in the second round of the major.\n",
      "It's been over a decade since Sharapova clinched her first Grand Slam championship. At age 17, she defeated Serena Williams in the 2004 Wimbledon final. It was her first of what would be five Grand Slam titles, and it came with a sizable paycheck: 560,500, or about $724,000.\n",
      "That's a lot of money for any teenager, particularly for one who grew up poor: At age seven, the Russian-born athlete and her dad arrived in the U.S. with just $700. While training at Nick Bollettieri's Tennis Academy, she slept on a pullout couch next to her dad in a $250-a-month apartment.\n",
      "\n",
      "And, when 17-year-old Sharapova secured her six-figure check, she headed straight to TJ Maxx.\n"
     ]
    }
   ],
   "source": [
    "# Just want to make sure that all fetched files contains the data\n",
    "# Here we have randomly taken the file\n",
    "\n",
    "file_to_open = os.path.join(folder_path, '15.txt.txt')\n",
    "with open(file_to_open, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "    print(file.read()[:1000])  # Print the first 1000 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert each words in the file to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01.txt.txt (lowercased): in 10 days, nasa's cassini spacecraft will nose-dive into saturn and burn up in the planet's atmosphere. it's the final, suicidal step of a months-long dance through saturn's rings that has given scie\n",
      "--------------------------------------------------\n",
      "02.txt.txt (lowercased): microsoft responded strongly to the trump administration's decision on tuesday to move toward rescinding or replacing daca within six months.\n",
      "\"there is nothing that we will be pushing on more strongly\n",
      "--------------------------------------------------\n",
      "03.txt.txt (lowercased): putin said that the development of ai raises both \"colossal opportunities\" and \"threats that are difficult to predict.\"\n",
      "\"whoever becomes the leader in this sphere will become the ruler of the world,\" \n",
      "--------------------------------------------------\n",
      "04.txt.txt (lowercased): meat that doesn't require the slaughter of animals. fashion that provides an eco-friendly alternative to leather. dairy products that are lactose-free and brewed in a lab.\n",
      "most san franciscans aren't \n",
      "--------------------------------------------------\n",
      "05.txt.txt (lowercased): so, first of all, i just want to say that in my view the equity analogy is incomplete. especially if doing a blockchain or token in the u.s., to be compliant with all regulations you need it to have a\n",
      "--------------------------------------------------\n",
      "06.txt.txt (lowercased): former us president barack obama decried his successor's decision to end an amnesty for 800,000 people brought to america illegally as children, describing it as \"wrong,\" \"self-defeating\" and \"cruel.\"\n",
      "--------------------------------------------------\n",
      "07.txt.txt (lowercased): bond yields are due to rise in a big way, according to strategist larry mcdonald of acg analytics.\n",
      "on tuesday, the 10-year u.s. treasury yield fell to 2.07 percent  its lowest level since november 201\n",
      "--------------------------------------------------\n",
      "08.txt.txt (lowercased): apple's next iphone cycle is expected to be one of the biggest yet  but that doesn't necessarily mean the stock will keep soaring, according to one of the company's most well-known followers.\n",
      "\n",
      "there c\n",
      "--------------------------------------------------\n",
      "09.txt.txt (lowercased): cassini launched on its billion-mile journey from earth to saturn on oct. 15, 1997. it was named for the astronomer giovanni cassini, who discovered four of the planet's moons and a gap in its rings. \n",
      "--------------------------------------------------\n",
      "10.txt.txt (lowercased): former president barack obama sharply criticized the trump administration's decision to end the daca program that protects young undocumented migrants from deportation, slamming the white house's move\n",
      "--------------------------------------------------\n",
      "11.txt.txt (lowercased): major league baseball confirmed that the boston red sox baseball team used an apple watch to steal hand signals from the new york yankees, according to the new york times.\n",
      "hand signals are used by tea\n",
      "--------------------------------------------------\n",
      "12.txt.txt (lowercased): socially responsible investing is attracting more dollars than ever. in fact, following the recent developments around the paris climate agreement, many investors are likely asking themselves how they\n",
      "--------------------------------------------------\n",
      "13.txt.txt (lowercased): china's yuan has been on a tear all year and has now recouped last year's losses  and analysts say there's still room to run.\n",
      "at its strongest level on tuesday, the yuan was changing hands at 6.5151 a\n",
      "--------------------------------------------------\n",
      "14.txt.txt (lowercased): a huge blackhole - about 100,000 times more massive than our sun - has been discovered lurking in a toxic gas cloud near the heart of the milky way.\n",
      "\n",
      "if confirmed, the object will rank as the second l\n",
      "--------------------------------------------------\n",
      "15.txt.txt (lowercased): after a 15-month suspension for drug use, maria sharapova returned to the u.s. open for the first time in three years on monday night. the 30-year-old put on a memorable performance under the lights o\n",
      "--------------------------------------------------\n",
      "16.txt.txt (lowercased): through the 1970s and 1980s, much of the social informat-\n",
      "ics research focused on organizations because they were\n",
      "the major sites of computerization. it is only in the last few\n",
      "years that many people \n",
      "--------------------------------------------------\n",
      "17.txt.txt (lowercased): hurricane irma, one of the most forceful atlantic storms in a century, churned across the ocean on tuesday on a collision course with puerto rico and the virgin islands, bearing down on the northern c\n",
      "--------------------------------------------------\n",
      "18.txt.txt (lowercased): according to jim cramer, \"there's always a bull market somewhere.\" even in the toughest markets, cramer's been able to find financial pockets of opportunity. he became well-known as one of the most su\n",
      "--------------------------------------------------\n",
      "19.txt.txt (lowercased): microsoft responded strongly to the trump administration's decision on tuesday to move toward rescinding or replacing daca within six months.\n",
      "\"there is nothing that we will be pushing on more strongly\n",
      "--------------------------------------------------\n",
      "20.txt.txt (lowercased): india's demonetized currency may have found its way back into the system but analysts suggest that far from tarnishing prime minister narendra modi's image, the strategy will ultimately be viewed as a\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Read each file, convert the content to lowercase, and print the first 200 characters to verify\n",
    "for file_name in file_names:\n",
    "    file_path = os.path.join(folder_path, file_name) # This will generate the full path for each file. \n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        content = file.read().lower()  # Converts the entire content of the file to lowercase.\n",
    "    \n",
    "    \n",
    "    print(f\"{file_name} (lowercased): {content[:200]}\\n{'-'*50}\") # Print the first 200 characters of each file, and we have added the dash (-) for better visibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a list with words from each text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01.txt.txt: ['in', '10', 'days', 'nasa', 's', 'cassini', 'spacecraft', 'will', 'nose', 'dive']\n",
      "02.txt.txt: ['microsoft', 'responded', 'strongly', 'to', 'the', 'trump', 'administration', 's', 'decision', 'on']\n",
      "03.txt.txt: ['putin', 'said', 'that', 'the', 'development', 'of', 'ai', 'raises', 'both', 'colossal']\n",
      "04.txt.txt: ['meat', 'that', 'doesn', 't', 'require', 'the', 'slaughter', 'of', 'animals', 'fashion']\n",
      "05.txt.txt: ['so', 'first', 'of', 'all', 'i', 'just', 'want', 'to', 'say', 'that']\n",
      "06.txt.txt: ['former', 'us', 'president', 'barack', 'obama', 'decried', 'his', 'successor', 's', 'decision']\n",
      "07.txt.txt: ['bond', 'yields', 'are', 'due', 'to', 'rise', 'in', 'a', 'big', 'way']\n",
      "08.txt.txt: ['apple', 's', 'next', 'iphone', 'cycle', 'is', 'expected', 'to', 'be', 'one']\n",
      "09.txt.txt: ['cassini', 'launched', 'on', 'its', 'billion', 'mile', 'journey', 'from', 'earth', 'to']\n",
      "10.txt.txt: ['former', 'president', 'barack', 'obama', 'sharply', 'criticized', 'the', 'trump', 'administration', 's']\n",
      "11.txt.txt: ['major', 'league', 'baseball', 'confirmed', 'that', 'the', 'boston', 'red', 'sox', 'baseball']\n",
      "12.txt.txt: ['socially', 'responsible', 'investing', 'is', 'attracting', 'more', 'dollars', 'than', 'ever', 'in']\n",
      "13.txt.txt: ['china', 's', 'yuan', 'has', 'been', 'on', 'a', 'tear', 'all', 'year']\n",
      "14.txt.txt: ['a', 'huge', 'blackhole', 'about', '100', '000', 'times', 'more', 'massive', 'than']\n",
      "15.txt.txt: ['after', 'a', '15', 'month', 'suspension', 'for', 'drug', 'use', 'maria', 'sharapova']\n",
      "16.txt.txt: ['through', 'the', '1970s', 'and', '1980s', 'much', 'of', 'the', 'social', 'informat']\n",
      "17.txt.txt: ['hurricane', 'irma', 'one', 'of', 'the', 'most', 'forceful', 'atlantic', 'storms', 'in']\n",
      "18.txt.txt: ['according', 'to', 'jim', 'cramer', 'there', 's', 'always', 'a', 'bull', 'market']\n",
      "19.txt.txt: ['microsoft', 'responded', 'strongly', 'to', 'the', 'trump', 'administration', 's', 'decision', 'on']\n",
      "20.txt.txt: ['india', 's', 'demonetized', 'currency', 'may', 'have', 'found', 'its', 'way', 'back']\n"
     ]
    }
   ],
   "source": [
    "import re # THis module allows to search, match and manipulate the string\n",
    "all_words = {} # This dictionary will store a list of words for each file\n",
    "\n",
    "# Read each file, convert the content to lowercase, and split it into words\n",
    "for file_name in file_names:\n",
    "    file_path = os.path.join(folder_path, file_name) # Same thing, repeating\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        content = file.read().lower()  # Convert to lowercase\n",
    "        \n",
    "        # Split content into words (using regular expression to remove punctuation)\n",
    "        words = re.findall(r'\\b\\w+\\b', content)\n",
    "        #\\b: Marks the start or end of a word, ensuring that words are detected properly.\n",
    "        #\\w+: Matches one or more word characters (letters, digits, numbers, and underscores).\n",
    "        #\\b: Ensures we capture the whole word, not part of a word.\n",
    "\n",
    "        # Store the list of words for each file in the dictionary\n",
    "        all_words[file_name] = words\n",
    "\n",
    "# Print the first 10 words of each file to verify\n",
    "for file_name, words in all_words.items():  # This retrieves the key-value pairs\n",
    "    print(f\"{file_name}: {words[:10]}\")  # Print the first 10 words as a sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets load the stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample stopwords: [\"isn't\", 'be', 'herself', 'do', 'a', 'your', \"hasn't\", 'she', 'further', 'the']\n"
     ]
    }
   ],
   "source": [
    "# Function to load stopwords\n",
    "def load_stopwords(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        return set(file.read().split())  # We are setting them in a set for faster access. You may have large file so you can use this method\n",
    "\n",
    "# Path to stopwords file\n",
    "stopwords_path = '/Volumes/Jagannath/Fall 2024/Cloud Computing/Assignment_1_data/stopwords.txt'\n",
    "\n",
    "# Load stopwords\n",
    "stopwords = load_stopwords(stopwords_path)\n",
    "\n",
    "# Print a sample of stopwords to verify\n",
    "print(\"Sample stopwords:\", list(stopwords)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove stop words from each list and get the final list of words for each text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01.txt.txt: ['10', 'days', 'nasa', 's', 'cassini', 'spacecraft', 'will', 'nose', 'dive', 'saturn']\n",
      "02.txt.txt: ['microsoft', 'responded', 'strongly', 'trump', 'administration', 's', 'decision', 'tuesday', 'move', 'toward']\n",
      "03.txt.txt: ['putin', 'said', 'development', 'ai', 'raises', 'colossal', 'opportunities', 'threats', 'difficult', 'predict']\n",
      "04.txt.txt: ['meat', 'doesn', 't', 'require', 'slaughter', 'animals', 'fashion', 'provides', 'eco', 'friendly']\n",
      "05.txt.txt: ['first', 'just', 'want', 'say', 'view', 'equity', 'analogy', 'incomplete', 'especially', 'blockchain']\n",
      "06.txt.txt: ['former', 'us', 'president', 'barack', 'obama', 'decried', 'successor', 's', 'decision', 'end']\n",
      "07.txt.txt: ['bond', 'yields', 'due', 'rise', 'big', 'way', 'according', 'strategist', 'larry', 'mcdonald']\n",
      "08.txt.txt: ['apple', 's', 'next', 'iphone', 'cycle', 'expected', 'one', 'biggest', 'yet', 'doesn']\n",
      "09.txt.txt: ['cassini', 'launched', 'billion', 'mile', 'journey', 'earth', 'saturn', 'oct', '15', '1997']\n",
      "10.txt.txt: ['former', 'president', 'barack', 'obama', 'sharply', 'criticized', 'trump', 'administration', 's', 'decision']\n",
      "11.txt.txt: ['major', 'league', 'baseball', 'confirmed', 'boston', 'red', 'sox', 'baseball', 'team', 'used']\n",
      "12.txt.txt: ['socially', 'responsible', 'investing', 'attracting', 'dollars', 'ever', 'fact', 'following', 'recent', 'developments']\n",
      "13.txt.txt: ['china', 's', 'yuan', 'tear', 'year', 'now', 'recouped', 'last', 'year', 's']\n",
      "14.txt.txt: ['huge', 'blackhole', '100', '000', 'times', 'massive', 'sun', 'discovered', 'lurking', 'toxic']\n",
      "15.txt.txt: ['15', 'month', 'suspension', 'drug', 'use', 'maria', 'sharapova', 'returned', 'u', 's']\n",
      "16.txt.txt: ['1970s', '1980s', 'much', 'social', 'informat', 'ics', 'research', 'focused', 'organizations', 'major']\n",
      "17.txt.txt: ['hurricane', 'irma', 'one', 'forceful', 'atlantic', 'storms', 'century', 'churned', 'across', 'ocean']\n",
      "18.txt.txt: ['according', 'jim', 'cramer', 's', 'always', 'bull', 'market', 'somewhere', 'even', 'toughest']\n",
      "19.txt.txt: ['microsoft', 'responded', 'strongly', 'trump', 'administration', 's', 'decision', 'tuesday', 'move', 'toward']\n",
      "20.txt.txt: ['india', 's', 'demonetized', 'currency', 'may', 'found', 'way', 'back', 'system', 'analysts']\n"
     ]
    }
   ],
   "source": [
    "final_words = {} # Dictionary to store the final list of words (after removing stopwords)\n",
    "# Remove stopwords from each file's word list\n",
    "for file_name, words in all_words.items():\n",
    "    \n",
    "    filtered_words = [word for word in words if word not in stopwords] # Filter out words that are in the stopwords set\n",
    "    final_words[file_name] = filtered_words # Store the filtered list of words in the final_words dictionary\n",
    "\n",
    "# Print the first 10 filtered words of each file to verify\n",
    "for file_name, words in final_words.items():\n",
    "    print(f\"{file_name}: {words[:10]}\")  # Print the first 10 words as a sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build dictionary for each word with KEY being the document ID (file name) and VALUE as frequency (number of times the word appears in that particular file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "[('01', 1), ('07', 1), ('08', 1), ('20', 1)]\n",
      "days\n",
      "[('01', 1)]\n",
      "nasa\n",
      "[('01', 1)]\n",
      "s\n",
      "[('01', 5), ('02', 2), ('03', 2), ('04', 3), ('05', 4), ('06', 3), ('07', 3), ('08', 5), ('09', 4), ('10', 4), ('12', 1), ('13', 7), ('15', 5), ('17', 2), ('18', 6), ('19', 2), ('20', 4)]\n",
      "cassini\n",
      "[('01', 2), ('09', 5)]\n",
      "spacecraft\n",
      "[('01', 1)]\n",
      "will\n",
      "[('01', 1), ('02', 1), ('03', 2), ('07', 1), ('08', 1), ('14', 2), ('16', 1), ('19', 1), ('20', 1)]\n",
      "nose\n",
      "[('01', 1)]\n",
      "dive\n",
      "[('01', 1)]\n",
      "saturn\n",
      "[('01', 3), ('09', 3)]\n",
      "burn\n",
      "[('01', 1)]\n",
      "planet\n",
      "[('01', 2), ('09', 1)]\n",
      "atmosphere\n",
      "[('01', 1)]\n",
      "final\n",
      "[('01', 1), ('15', 1)]\n",
      "suicidal\n",
      "[('01', 1)]\n",
      "step\n",
      "[('01', 1), ('06', 1)]\n",
      "months\n",
      "[('01', 1), ('02', 1), ('08', 2), ('09', 1), ('19', 1), ('20', 1)]\n",
      "long\n",
      "[('01', 1)]\n",
      "dance\n",
      "[('01', 1)]\n",
      "rings\n",
      "[('01', 1), ('09', 1)]\n",
      "given\n",
      "[('01', 1), ('07', 1)]\n",
      "scientists\n",
      "[('01', 1), ('04', 1), ('14', 1)]\n",
      "unprecedented\n",
      "[('01', 1)]\n",
      "view\n",
      "[('01', 1), ('05', 1), ('07', 1)]\n",
      "sixth\n",
      "[('01', 1)]\n",
      "sun\n",
      "[('01', 1), ('14', 1)]\n",
      "also\n",
      "[('01', 1), ('09', 1), ('14', 1)]\n",
      "end\n",
      "[('01', 2), ('06', 1), ('10', 1)]\n",
      "mission\n",
      "[('01', 1)]\n",
      "revolutionized\n",
      "[('01', 1)]\n",
      "understanding\n",
      "[('01', 1), ('16', 1)]\n",
      "opened\n",
      "[('01', 1)]\n",
      "eyes\n",
      "[('01', 1)]\n",
      "two\n",
      "[('01', 1)]\n",
      "worlds\n",
      "[('01', 1)]\n",
      "home\n",
      "[('01', 1), ('10', 1), ('16', 3)]\n",
      "alien\n",
      "[('01', 1)]\n",
      "life\n",
      "[('01', 1)]\n",
      "moons\n",
      "[('01', 1), ('09', 1)]\n",
      "titan\n",
      "[('01', 1), ('09', 2)]\n",
      "enceladus\n",
      "[('01', 1)]\n",
      "really\n",
      "[('01', 1)]\n",
      "era\n",
      "[('01', 1), ('16', 1)]\n",
      "fans\n",
      "[('01', 1)]\n",
      "devastated\n",
      "[('01', 1)]\n",
      "microsoft\n",
      "[('02', 4), ('03', 1), ('19', 4)]\n",
      "responded\n",
      "[('02', 1), ('19', 1)]\n",
      "strongly\n",
      "[('02', 2), ('19', 2)]\n",
      "trump\n",
      "[('02', 1), ('06', 2), ('10', 1), ('19', 1)]\n",
      "administration\n",
      "[('02', 1), ('10', 2), ('19', 1)]\n",
      "decision\n",
      "[('02', 1), ('06', 2), ('10', 3), ('19', 1)]\n",
      "tuesday\n",
      "[('02', 1), ('07', 2), ('08', 1), ('13', 1), ('17', 2), ('19', 1)]\n",
      "move\n",
      "[('02', 2), ('06', 1), ('07', 1), ('10', 1), ('19', 2), ('20', 2)]\n",
      "toward\n",
      "[('02', 1), ('19', 1)]\n",
      "rescinding\n",
      "[('02', 1), ('19', 1)]\n",
      "replacing\n",
      "[('02', 1), ('19', 1)]\n",
      "daca\n",
      "[('02', 1), ('06', 1), ('10', 1), ('19', 1)]\n",
      "within\n",
      "[('02', 1), ('19', 1)]\n",
      "six\n",
      "[('02', 1), ('15', 1), ('19', 1)]\n",
      "nothing\n",
      "[('02', 1), ('06', 1), ('19', 1)]\n",
      "pushing\n",
      "[('02', 1), ('13', 1), ('19', 1)]\n",
      "congress\n",
      "[('02', 1), ('06', 1), ('10', 1), ('19', 1)]\n",
      "act\n",
      "[('02', 1), ('10', 1), ('19', 1)]\n",
      "president\n",
      "[('02', 1), ('06', 1), ('10', 2), ('19', 1)]\n",
      "chief\n",
      "[('02', 1), ('19', 1)]\n",
      "legal\n",
      "[('02', 1), ('10', 1), ('19', 1)]\n",
      "officer\n",
      "[('02', 1), ('19', 1)]\n",
      "brad\n",
      "[('02', 1), ('19', 1)]\n",
      "smith\n",
      "[('02', 5), ('19', 5)]\n",
      "said\n",
      "[('02', 3), ('03', 3), ('05', 1), ('06', 1), ('07', 1), ('08', 1), ('10', 1), ('14', 2), ('19', 3)]\n",
      "interview\n",
      "[('02', 1), ('05', 1), ('19', 1)]\n",
      "npr\n",
      "[('02', 1), ('19', 1)]\n",
      "put\n",
      "[('02', 1), ('15', 1), ('19', 1)]\n",
      "stake\n",
      "[('02', 1), ('19', 1)]\n",
      "ground\n",
      "[('02', 1), ('19', 1)]\n",
      "care\n",
      "[('02', 1), ('19', 1)]\n",
      "tax\n",
      "[('02', 1), ('19', 1)]\n",
      "reform\n",
      "[('02', 1), ('19', 1)]\n",
      "bill\n",
      "[('02', 1), ('04', 1), ('19', 1)]\n",
      "noting\n",
      "[('02', 1), ('19', 1)]\n",
      "entire\n",
      "[('02', 1), ('18', 1), ('19', 1)]\n",
      "business\n",
      "[('02', 1), ('19', 1)]\n",
      "community\n",
      "[('02', 1), ('19', 1)]\n",
      "cares\n",
      "[('02', 1), ('19', 1)]\n",
      "one\n",
      "[('02', 1), ('04', 1), ('05', 2), ('08', 3), ('09', 1), ('15', 1), ('17', 1), ('18', 1), ('19', 1), ('20', 2)]\n",
      "needs\n",
      "[('02', 1), ('06', 1), ('19', 1)]\n",
      "settled\n",
      "[('02', 1), ('19', 1)]\n",
      "first\n",
      "[('02', 1), ('04', 1), ('05', 1), ('09', 4), ('14', 1), ('15', 3), ('19', 1)]\n",
      "added\n",
      "[('02', 1), ('19', 1)]\n",
      "won\n",
      "[('02', 1), ('05', 1), ('19', 1)]\n",
      "t\n",
      "[('02', 1), ('04', 2), ('05', 2), ('08', 1), ('19', 1)]\n",
      "easy\n",
      "[('02', 1), ('19', 1)]\n",
      "government\n",
      "[('02', 2), ('03', 1), ('19', 2)]\n",
      "deport\n",
      "[('02', 1), ('19', 1)]\n",
      "employees\n",
      "[('02', 1), ('19', 1)]\n",
      "dreamers\n",
      "[('02', 1), ('19', 1)]\n",
      "going\n",
      "[('02', 1), ('08', 1), ('11', 2), ('19', 1)]\n",
      "go\n",
      "[('02', 1), ('19', 1)]\n",
      "us\n",
      "[('02', 1), ('06', 2), ('12', 1), ('19', 1)]\n",
      "get\n",
      "[('02', 1), ('18', 1), ('19', 1)]\n",
      "person\n",
      "[('02', 1), ('19', 1)]\n",
      "latest\n",
      "[('02', 1), ('19', 1)]\n",
      "tech\n",
      "[('02', 1), ('03', 1), ('19', 1)]\n",
      "company\n",
      "[('02', 1), ('08', 1), ('19', 1)]\n",
      "speak\n",
      "[('02', 1), ('19', 1)]\n",
      "made\n",
      "[('02', 1), ('19', 1)]\n",
      "similar\n",
      "[('02', 1), ('19', 1)]\n",
      "comments\n",
      "[('02', 1), ('19', 1)]\n",
      "public\n",
      "[('02', 1), ('16', 1), ('19', 1)]\n",
      "letter\n",
      "[('02', 1), ('19', 1)]\n",
      "putin\n",
      "[('03', 1)]\n",
      "development\n",
      "[('03', 2)]\n",
      "ai\n",
      "[('03', 5)]\n",
      "raises\n",
      "[('03', 1), ('16', 1)]\n",
      "colossal\n",
      "[('03', 1)]\n",
      "opportunities\n",
      "[('03', 1)]\n",
      "threats\n",
      "[('03', 1)]\n",
      "difficult\n",
      "[('03', 1), ('08', 1)]\n",
      "predict\n",
      "[('03', 1)]\n",
      "whoever\n",
      "[('03', 1)]\n",
      "becomes\n",
      "[('03', 1)]\n",
      "leader\n",
      "[('03', 1)]\n",
      "sphere\n",
      "[('03', 1)]\n",
      "become\n",
      "[('03', 2)]\n",
      "ruler\n",
      "[('03', 1)]\n",
      "world\n",
      "[('03', 1)]\n",
      "meeting\n",
      "[('03', 1)]\n",
      "students\n",
      "[('03', 1)]\n",
      "friday\n",
      "[('03', 1), ('07', 1)]\n",
      "united\n",
      "[('03', 1), ('10', 2)]\n",
      "states\n",
      "[('03', 1), ('10', 2)]\n",
      "generally\n",
      "[('03', 1), ('07', 1)]\n",
      "considered\n",
      "[('03', 1)]\n",
      "nation\n",
      "[('03', 1), ('07', 1)]\n",
      "leading\n",
      "[('03', 1)]\n",
      "charge\n",
      "[('03', 1)]\n",
      "towards\n",
      "[('03', 1), ('14', 1)]\n",
      "currently\n",
      "[('03', 1)]\n",
      "giants\n",
      "[('03', 1)]\n",
      "like\n",
      "[('03', 1), ('05', 1)]\n",
      "google\n",
      "[('03', 1)]\n",
      "pouring\n",
      "[('03', 1)]\n",
      "large\n",
      "[('03', 1)]\n",
      "amounts\n",
      "[('03', 1)]\n",
      "cash\n",
      "[('03', 1)]\n",
      "research\n",
      "[('03', 1), ('14', 1), ('16', 1)]\n",
      "projects\n",
      "[('03', 1)]\n",
      "last\n",
      "[('03', 1), ('13', 1), ('16', 1)]\n",
      "week\n",
      "[('03', 1), ('20', 1)]\n",
      "report\n",
      "[('03', 1), ('20', 1)]\n",
      "goldman\n",
      "[('03', 1)]\n",
      "sachs\n",
      "[('03', 1)]\n",
      "found\n",
      "[('03', 1), ('14', 2), ('20', 2)]\n",
      "china\n",
      "[('03', 1), ('13', 1)]\n",
      "capability\n",
      "[('03', 1)]\n",
      "catch\n",
      "[('03', 1)]\n",
      "u\n",
      "[('03', 1), ('05', 1), ('07', 1), ('12', 1), ('13', 2), ('15', 2), ('17', 2)]\n",
      "believe\n",
      "[('03', 1)]\n",
      "technology\n",
      "[('03', 1), ('04', 1)]\n",
      "priority\n",
      "[('03', 1)]\n",
      "agenda\n",
      "[('03', 1)]\n",
      "expect\n",
      "[('03', 1), ('08', 1)]\n",
      "national\n",
      "[('03', 1), ('06', 1), ('17', 1)]\n",
      "regional\n",
      "[('03', 1)]\n",
      "policy\n",
      "[('03', 1)]\n",
      "funding\n",
      "[('03', 1)]\n",
      "support\n",
      "[('03', 1)]\n",
      "follow\n",
      "[('03', 1)]\n",
      "investment\n",
      "[('03', 1), ('12', 2)]\n",
      "bank\n",
      "[('03', 1), ('13', 1), ('20', 2)]\n",
      "meat\n",
      "[('04', 3)]\n",
      "doesn\n",
      "[('04', 1), ('08', 1)]\n",
      "require\n",
      "[('04', 1)]\n",
      "slaughter\n",
      "[('04', 1)]\n",
      "animals\n",
      "[('04', 1)]\n",
      "fashion\n",
      "[('04', 1)]\n",
      "provides\n",
      "[('04', 1)]\n",
      "eco\n",
      "[('04', 1)]\n",
      "friendly\n",
      "[('04', 1)]\n",
      "alternative\n",
      "[('04', 1)]\n",
      "leather\n",
      "[('04', 1)]\n",
      "dairy\n",
      "[('04', 1)]\n",
      "products\n",
      "[('04', 1)]\n",
      "lactose\n",
      "[('04', 1)]\n",
      "free\n",
      "[('04', 1)]\n",
      "brewed\n",
      "[('04', 2)]\n",
      "lab\n",
      "[('04', 3)]\n",
      "san\n",
      "[('04', 1)]\n",
      "franciscans\n",
      "[('04', 1)]\n",
      "aren\n",
      "[('04', 1), ('05', 1)]\n",
      "aware\n",
      "[('04', 1)]\n",
      "heart\n",
      "[('04', 1), ('14', 2)]\n",
      "financial\n",
      "[('04', 1), ('18', 1)]\n",
      "district\n",
      "[('04', 1)]\n",
      "entrepreneurs\n",
      "[('04', 1)]\n",
      "working\n",
      "[('04', 1)]\n",
      "things\n",
      "[('04', 1), ('05', 1)]\n",
      "run\n",
      "[('04', 1), ('08', 1), ('13', 1)]\n",
      "bio\n",
      "[('04', 1)]\n",
      "accelerator\n",
      "[('04', 1)]\n",
      "program\n",
      "[('04', 1), ('10', 1)]\n",
      "called\n",
      "[('04', 1)]\n",
      "indiebio\n",
      "[('04', 2)]\n",
      "funded\n",
      "[('04', 1)]\n",
      "68\n",
      "[('04', 1)]\n",
      "early\n",
      "[('04', 1), ('17', 1)]\n",
      "stage\n",
      "[('04', 1), ('06', 1)]\n",
      "start\n",
      "[('04', 1), ('06', 1)]\n",
      "ups\n",
      "[('04', 1), ('08', 1)]\n",
      "biggest\n",
      "[('04', 1), ('08', 2)]\n",
      "successes\n",
      "[('04', 1)]\n",
      "memphis\n",
      "[('04', 2)]\n",
      "meats\n",
      "[('04', 1)]\n",
      "raised\n",
      "[('04', 1)]\n",
      "17\n",
      "[('04', 1), ('15', 2)]\n",
      "million\n",
      "[('04', 1)]\n",
      "past\n",
      "[('04', 1), ('08', 1)]\n",
      "month\n",
      "[('04', 1), ('13', 1), ('15', 2)]\n",
      "clean\n",
      "[('04', 1)]\n",
      "bioreactor\n",
      "[('04', 1)]\n",
      "slew\n",
      "[('04', 1)]\n",
      "high\n",
      "[('04', 1), ('20', 1)]\n",
      "profile\n",
      "[('04', 1)]\n",
      "investors\n",
      "[('04', 1), ('12', 2), ('18', 1)]\n",
      "including\n",
      "[('04', 1)]\n",
      "billionaires\n",
      "[('04', 1)]\n",
      "gates\n",
      "[('04', 1)]\n",
      "richard\n",
      "[('04', 1)]\n",
      "branson\n",
      "[('04', 1)]\n",
      "elon\n",
      "[('04', 1)]\n",
      "musk\n",
      "[('04', 1)]\n",
      "brother\n",
      "[('04', 1)]\n",
      "kimball\n",
      "[('04', 1)]\n",
      "general\n",
      "[('04', 1), ('05', 1), ('06', 1)]\n",
      "electric\n",
      "[('04', 1)]\n",
      "ceo\n",
      "[('04', 1)]\n",
      "jack\n",
      "[('04', 1)]\n",
      "welch\n",
      "[('04', 1)]\n",
      "investor\n",
      "[('04', 1)]\n",
      "just\n",
      "[('05', 2), ('08', 1), ('15', 1), ('20', 1)]\n",
      "want\n",
      "[('05', 2), ('06', 1)]\n",
      "say\n",
      "[('05', 1), ('07', 1), ('13', 2)]\n",
      "equity\n",
      "[('05', 1)]\n",
      "analogy\n",
      "[('05', 1)]\n",
      "incomplete\n",
      "[('05', 1)]\n",
      "especially\n",
      "[('05', 1)]\n",
      "blockchain\n",
      "[('05', 2)]\n",
      "token\n",
      "[('05', 3)]\n",
      "compliant\n",
      "[('05', 1)]\n",
      "regulations\n",
      "[('05', 1)]\n",
      "need\n",
      "[('05', 1)]\n",
      "utility\n",
      "[('05', 1)]\n",
      "component\n",
      "[('05', 1)]\n",
      "use\n",
      "[('05', 1), ('10', 1), ('11', 1), ('12', 1), ('15', 1), ('16', 1)]\n",
      "proxy\n",
      "[('05', 1)]\n",
      "equities\n",
      "[('05', 3)]\n",
      "hard\n",
      "[('05', 1)]\n",
      "many\n",
      "[('05', 1), ('07', 1), ('12', 1), ('16', 1)]\n",
      "interesting\n",
      "[('05', 1)]\n",
      "can\n",
      "[('05', 2), ('06', 1), ('12', 1), ('13', 1), ('18', 3)]\n",
      "tokenized\n",
      "[('05', 1)]\n",
      "networks\n",
      "[('05', 1)]\n",
      "stickler\n",
      "[('05', 1)]\n",
      "every\n",
      "[('05', 1)]\n",
      "point\n",
      "[('05', 1)]\n",
      "making\n",
      "[('05', 1), ('14', 1)]\n",
      "distinction\n",
      "[('05', 1)]\n",
      "important\n",
      "[('05', 1), ('12', 1), ('20', 1)]\n",
      "keep\n",
      "[('05', 1), ('08', 1)]\n",
      "mind\n",
      "[('05', 1)]\n",
      "work\n",
      "[('05', 1), ('16', 1)]\n",
      "lawyers\n",
      "[('05', 1)]\n",
      "ensure\n",
      "[('05', 1)]\n",
      "tokens\n",
      "[('05', 1)]\n",
      "based\n",
      "[('05', 1), ('07', 1), ('20', 1)]\n",
      "assets\n",
      "[('05', 1), ('07', 1), ('12', 1)]\n",
      "relative\n",
      "[('05', 1)]\n",
      "existing\n",
      "[('05', 1)]\n",
      "secondary\n",
      "[('05', 2)]\n",
      "markets\n",
      "[('05', 2), ('18', 1)]\n",
      "either\n",
      "[('05', 1)]\n",
      "property\n",
      "[('05', 1)]\n",
      "houses\n",
      "[('05', 1)]\n",
      "scale\n",
      "[('05', 1)]\n",
      "happening\n",
      "[('05', 1)]\n",
      "market\n",
      "[('05', 2), ('18', 1)]\n",
      "unparalleled\n",
      "[('05', 1)]\n",
      "thing\n",
      "[('05', 1), ('08', 1)]\n",
      "bunch\n",
      "[('05', 1)]\n",
      "small\n",
      "[('05', 1)]\n",
      "quite\n",
      "[('05', 1)]\n",
      "another\n",
      "[('05', 1)]\n",
      "turn\n",
      "[('05', 1)]\n",
      "internet\n",
      "[('05', 2), ('16', 2)]\n",
      "anyone\n",
      "[('05', 1)]\n",
      "connection\n",
      "[('05', 1)]\n",
      "purchase\n",
      "[('05', 1)]\n",
      "see\n",
      "[('05', 1), ('18', 1)]\n",
      "shapeshift\n",
      "[('05', 1)]\n",
      "io\n",
      "[('05', 1)]\n",
      "great\n",
      "[('05', 1)]\n",
      "example\n",
      "[('05', 1), ('11', 1)]\n",
      "former\n",
      "[('06', 1), ('10', 2)]\n",
      "barack\n",
      "[('06', 1), ('10', 1)]\n",
      "obama\n",
      "[('06', 2), ('10', 3)]\n",
      "decried\n",
      "[('06', 1)]\n",
      "successor\n",
      "[('06', 1)]\n",
      "amnesty\n",
      "[('06', 2)]\n",
      "800\n",
      "[('06', 1), ('10', 1)]\n",
      "000\n",
      "[('06', 1), ('10', 1), ('14', 1), ('15', 1)]\n",
      "people\n",
      "[('06', 2), ('16', 1)]\n",
      "brought\n",
      "[('06', 1), ('10', 2)]\n",
      "america\n",
      "[('06', 1)]\n",
      "illegally\n",
      "[('06', 1)]\n",
      "children\n",
      "[('06', 1), ('10', 1)]\n",
      "describing\n",
      "[('06', 1)]\n",
      "wrong\n",
      "[('06', 3)]\n",
      "self\n",
      "[('06', 2), ('10', 1)]\n",
      "defeating\n",
      "[('06', 2), ('10', 1)]\n",
      "cruel\n",
      "[('06', 2), ('10', 1)]\n",
      "rare\n",
      "[('06', 1)]\n",
      "re\n",
      "[('06', 1), ('10', 1)]\n",
      "entry\n",
      "[('06', 1)]\n",
      "onto\n",
      "[('06', 1)]\n",
      "political\n",
      "[('06', 1), ('10', 1), ('13', 1), ('20', 1)]\n",
      "used\n",
      "[('06', 1), ('11', 2)]\n",
      "facebook\n",
      "[('06', 1), ('10', 1)]\n",
      "post\n",
      "[('06', 1), ('10', 1)]\n",
      "slam\n",
      "[('06', 1), ('15', 2)]\n",
      "call\n",
      "[('06', 1)]\n",
      "target\n",
      "[('06', 1)]\n",
      "young\n",
      "[('06', 1), ('10', 1)]\n",
      "done\n",
      "[('06', 1)]\n",
      "new\n",
      "[('06', 1), ('11', 3), ('20', 2)]\n",
      "businesses\n",
      "[('06', 1)]\n",
      "staff\n",
      "[('06', 1), ('11', 2)]\n",
      "labs\n",
      "[('06', 1)]\n",
      "serve\n",
      "[('06', 1)]\n",
      "military\n",
      "[('06', 1)]\n",
      "otherwise\n",
      "[('06', 1)]\n",
      "contribute\n",
      "[('06', 1)]\n",
      "country\n",
      "[('06', 3), ('10', 2), ('20', 1)]\n",
      "love\n",
      "[('06', 1)]\n",
      "wrote\n",
      "[('06', 1), ('10', 1)]\n",
      "donald\n",
      "[('06', 1)]\n",
      "announcement\n",
      "[('06', 1)]\n",
      "scrap\n",
      "[('06', 1)]\n",
      "programme\n",
      "[('06', 1)]\n",
      "met\n",
      "[('06', 1)]\n",
      "protests\n",
      "[('06', 1)]\n",
      "across\n",
      "[('06', 1), ('14', 1), ('17', 1)]\n",
      "hundreds\n",
      "[('06', 1)]\n",
      "gathered\n",
      "[('06', 1)]\n",
      "outside\n",
      "[('06', 1)]\n",
      "white\n",
      "[('06', 1), ('10', 1)]\n",
      "house\n",
      "[('06', 1), ('10', 1)]\n",
      "protest\n",
      "[('06', 1)]\n",
      "attorney\n",
      "[('06', 1)]\n",
      "jeff\n",
      "[('06', 1)]\n",
      "sessions\n",
      "[('06', 1)]\n",
      "defending\n",
      "[('06', 1)]\n",
      "lawful\n",
      "[('06', 1)]\n",
      "system\n",
      "[('06', 1), ('09', 1), ('20', 1)]\n",
      "immigration\n",
      "[('06', 1)]\n",
      "serves\n",
      "[('06', 1)]\n",
      "interest\n",
      "[('06', 1), ('12', 1)]\n",
      "everyone\n",
      "[('06', 1)]\n",
      "admitted\n",
      "[('06', 1)]\n",
      "bond\n",
      "[('07', 2)]\n",
      "yields\n",
      "[('07', 7)]\n",
      "due\n",
      "[('07', 1)]\n",
      "rise\n",
      "[('07', 1)]\n",
      "big\n",
      "[('07', 1)]\n",
      "way\n",
      "[('07', 1), ('14', 6), ('20', 2)]\n",
      "according\n",
      "[('07', 1), ('08', 1), ('11', 2), ('12', 1), ('13', 1), ('14', 1), ('18', 1)]\n",
      "strategist\n",
      "[('07', 1)]\n",
      "larry\n",
      "[('07', 1)]\n",
      "mcdonald\n",
      "[('07', 3)]\n",
      "acg\n",
      "[('07', 1)]\n",
      "analytics\n",
      "[('07', 1)]\n",
      "year\n",
      "[('07', 1), ('13', 3), ('15', 2)]\n",
      "treasury\n",
      "[('07', 1)]\n",
      "yield\n",
      "[('07', 1)]\n",
      "fell\n",
      "[('07', 1)]\n",
      "2\n",
      "[('07', 1), ('15', 1)]\n",
      "07\n",
      "[('07', 1)]\n",
      "percent\n",
      "[('07', 2), ('08', 1), ('12', 1), ('13', 1), ('20', 1)]\n",
      "lowest\n",
      "[('07', 1)]\n",
      "level\n",
      "[('07', 2), ('13', 1)]\n",
      "since\n",
      "[('07', 2), ('15', 1), ('20', 1)]\n",
      "november\n",
      "[('07', 1)]\n",
      "2016\n",
      "[('07', 1), ('12', 1)]\n",
      "economy\n",
      "[('07', 2)]\n",
      "continues\n",
      "[('07', 1), ('12', 1)]\n",
      "firm\n",
      "[('07', 1), ('08', 1)]\n",
      "footing\n",
      "[('07', 1)]\n",
      "reflects\n",
      "[('07', 1)]\n",
      "global\n",
      "[('07', 1)]\n",
      "jitters\n",
      "[('07', 1)]\n",
      "geopolitical\n",
      "[('07', 1)]\n",
      "risk\n",
      "[('07', 1)]\n",
      "coming\n",
      "[('07', 1), ('14', 1)]\n",
      "north\n",
      "[('07', 1), ('13', 1)]\n",
      "korea\n",
      "[('07', 1), ('13', 1)]\n",
      "driving\n",
      "[('07', 1)]\n",
      "much\n",
      "[('07', 2), ('13', 1), ('16', 1)]\n",
      "lower\n",
      "[('07', 2)]\n",
      "cnbc\n",
      "[('07', 1), ('08', 1)]\n",
      "trading\n",
      "[('07', 1), ('08', 1), ('18', 1)]\n",
      "driven\n",
      "[('07', 1)]\n",
      "unsustainably\n",
      "[('07', 1)]\n",
      "reflect\n",
      "[('07', 1)]\n",
      "strength\n",
      "[('07', 1)]\n",
      "greater\n",
      "[('07', 1)]\n",
      "perceived\n",
      "[('07', 1)]\n",
      "chance\n",
      "[('07', 1)]\n",
      "earning\n",
      "[('07', 1)]\n",
      "money\n",
      "[('07', 1), ('12', 1), ('15', 1), ('18', 5), ('20', 2)]\n",
      "higher\n",
      "[('07', 2)]\n",
      "expected\n",
      "[('07', 1), ('08', 1), ('13', 1), ('17', 1)]\n",
      "rate\n",
      "[('07', 1)]\n",
      "inflation\n",
      "[('07', 1)]\n",
      "loftier\n",
      "[('07', 1)]\n",
      "tend\n",
      "[('07', 1)]\n",
      "metric\n",
      "[('07', 1)]\n",
      "strikingly\n",
      "[('07', 1)]\n",
      "low\n",
      "[('07', 1)]\n",
      "indeed\n",
      "[('07', 1)]\n",
      "points\n",
      "[('07', 1)]\n",
      "reading\n",
      "[('07', 1)]\n",
      "institute\n",
      "[('07', 1)]\n",
      "supply\n",
      "[('07', 1)]\n",
      "management\n",
      "[('07', 1), ('12', 1), ('18', 1)]\n",
      "manufacturing\n",
      "[('07', 1)]\n",
      "index\n",
      "[('07', 1)]\n",
      "came\n",
      "[('07', 1), ('15', 1), ('20', 1)]\n",
      "highest\n",
      "[('07', 1)]\n",
      "2011\n",
      "[('07', 1)]\n",
      "back\n",
      "[('07', 1), ('20', 1)]\n",
      "1\n",
      "[('07', 1), ('14', 1)]\n",
      "5\n",
      "[('07', 1), ('17', 1)]\n",
      "now\n",
      "[('07', 1), ('13', 1), ('18', 1)]\n",
      "noted\n",
      "[('07', 1), ('20', 1)]\n",
      "apple\n",
      "[('08', 3), ('11', 2)]\n",
      "next\n",
      "[('08', 2), ('15', 1)]\n",
      "iphone\n",
      "[('08', 1)]\n",
      "cycle\n",
      "[('08', 2)]\n",
      "yet\n",
      "[('08', 1)]\n",
      "necessarily\n",
      "[('08', 1)]\n",
      "mean\n",
      "[('08', 1)]\n",
      "stock\n",
      "[('08', 1), ('18', 1)]\n",
      "soaring\n",
      "[('08', 1)]\n",
      "well\n",
      "[('08', 1), ('10', 1), ('18', 2)]\n",
      "known\n",
      "[('08', 1), ('18', 1)]\n",
      "followers\n",
      "[('08', 1)]\n",
      "pullback\n",
      "[('08', 1)]\n",
      "shares\n",
      "[('08', 1)]\n",
      "three\n",
      "[('08', 2), ('15', 1)]\n",
      "gene\n",
      "[('08', 1)]\n",
      "munster\n",
      "[('08', 2)]\n",
      "told\n",
      "[('08', 1), ('14', 1)]\n",
      "power\n",
      "[('08', 1)]\n",
      "lunch\n",
      "[('08', 1)]\n",
      "history\n",
      "[('08', 1), ('18', 1)]\n",
      "four\n",
      "[('08', 1), ('09', 1)]\n",
      "years\n",
      "[('08', 2), ('09', 1), ('14', 2), ('15', 1), ('16', 1), ('18', 2)]\n",
      "little\n",
      "[('08', 1)]\n",
      "bit\n",
      "[('08', 1)]\n",
      "look\n",
      "[('08', 1)]\n",
      "happens\n",
      "[('08', 1)]\n",
      "product\n",
      "[('08', 2), ('18', 1)]\n",
      "announced\n",
      "[('08', 1)]\n",
      "covered\n",
      "[('08', 1)]\n",
      "wall\n",
      "[('08', 1), ('18', 2)]\n",
      "street\n",
      "[('08', 1), ('18', 2)]\n",
      "analyst\n",
      "[('08', 1), ('20', 1)]\n",
      "becoming\n",
      "[('08', 1)]\n",
      "managing\n",
      "[('08', 1), ('18', 1)]\n",
      "partner\n",
      "[('08', 1)]\n",
      "venture\n",
      "[('08', 1)]\n",
      "capital\n",
      "[('08', 1), ('13', 1)]\n",
      "loup\n",
      "[('08', 1)]\n",
      "ventures\n",
      "[('08', 1)]\n",
      "suggest\n",
      "[('08', 1), ('20', 1)]\n",
      "typically\n",
      "[('08', 1), ('11', 1)]\n",
      "tail\n",
      "[('08', 1)]\n",
      "launched\n",
      "[('09', 1)]\n",
      "billion\n",
      "[('09', 1), ('20', 1)]\n",
      "mile\n",
      "[('09', 1)]\n",
      "journey\n",
      "[('09', 1)]\n",
      "earth\n",
      "[('09', 1)]\n",
      "oct\n",
      "[('09', 1)]\n",
      "15\n",
      "[('09', 1), ('15', 1), ('20', 1)]\n",
      "1997\n",
      "[('09', 1)]\n",
      "named\n",
      "[('09', 2)]\n",
      "astronomer\n",
      "[('09', 1), ('14', 1)]\n",
      "giovanni\n",
      "[('09', 1)]\n",
      "discovered\n",
      "[('09', 1), ('14', 1)]\n",
      "gap\n",
      "[('09', 1)]\n",
      "carried\n",
      "[('09', 1)]\n",
      "single\n",
      "[('09', 1)]\n",
      "passenger\n",
      "[('09', 1)]\n",
      "huygens\n",
      "[('09', 3)]\n",
      "lander\n",
      "[('09', 1)]\n",
      "built\n",
      "[('09', 1)]\n",
      "european\n",
      "[('09', 1)]\n",
      "space\n",
      "[('09', 1)]\n",
      "agency\n",
      "[('09', 1)]\n",
      "dutch\n",
      "[('09', 1)]\n",
      "scientist\n",
      "[('09', 1)]\n",
      "spotted\n",
      "[('09', 1)]\n",
      "moon\n",
      "[('09', 2)]\n",
      "arrived\n",
      "[('09', 1), ('15', 1)]\n",
      "orbit\n",
      "[('09', 2)]\n",
      "seven\n",
      "[('09', 1), ('15', 1)]\n",
      "launch\n",
      "[('09', 1)]\n",
      "july\n",
      "[('09', 1)]\n",
      "2004\n",
      "[('09', 1), ('15', 1)]\n",
      "several\n",
      "[('09', 1)]\n",
      "later\n",
      "[('09', 1)]\n",
      "split\n",
      "[('09', 1)]\n",
      "touched\n",
      "[('09', 1)]\n",
      "shore\n",
      "[('09', 1)]\n",
      "lakes\n",
      "[('09', 1)]\n",
      "liquid\n",
      "[('09', 1)]\n",
      "methane\n",
      "[('09', 1)]\n",
      "humankind\n",
      "[('09', 1)]\n",
      "ever\n",
      "[('09', 1), ('12', 1)]\n",
      "landing\n",
      "[('09', 2)]\n",
      "kind\n",
      "[('09', 1), ('11', 1)]\n",
      "outer\n",
      "[('09', 1)]\n",
      "solar\n",
      "[('09', 1)]\n",
      "meanwhile\n",
      "[('09', 1)]\n",
      "probe\n",
      "[('09', 1)]\n",
      "pioneer\n",
      "[('09', 1)]\n",
      "voyager\n",
      "[('09', 1)]\n",
      "simply\n",
      "[('09', 1)]\n",
      "flown\n",
      "[('09', 1)]\n",
      "1979\n",
      "[('09', 1)]\n",
      "1980\n",
      "[('09', 1)]\n",
      "respectively\n",
      "[('09', 1)]\n",
      "sharply\n",
      "[('10', 1)]\n",
      "criticized\n",
      "[('10', 1)]\n",
      "protects\n",
      "[('10', 1)]\n",
      "undocumented\n",
      "[('10', 3)]\n",
      "migrants\n",
      "[('10', 1)]\n",
      "deportation\n",
      "[('10', 1)]\n",
      "slamming\n",
      "[('10', 1)]\n",
      "legally\n",
      "[('10', 1)]\n",
      "unnecessary\n",
      "[('10', 1)]\n",
      "page\n",
      "[('10', 1)]\n",
      "defended\n",
      "[('10', 1)]\n",
      "shield\n",
      "[('10', 1)]\n",
      "immigrants\n",
      "[('10', 1)]\n",
      "decided\n",
      "[('10', 1)]\n",
      "established\n",
      "[('10', 1)]\n",
      "principle\n",
      "[('10', 1)]\n",
      "discretion\n",
      "[('10', 1)]\n",
      "protect\n",
      "[('10', 1)]\n",
      "individuals\n",
      "[('10', 1)]\n",
      "know\n",
      "[('10', 3), ('11', 1)]\n",
      "failed\n",
      "[('10', 1)]\n",
      "parents\n",
      "[('10', 1)]\n",
      "sometimes\n",
      "[('10', 1)]\n",
      "even\n",
      "[('10', 2), ('13', 1), ('14', 1), ('18', 1)]\n",
      "infants\n",
      "[('10', 1)]\n",
      "may\n",
      "[('10', 2), ('11', 1), ('20', 1)]\n",
      "besides\n",
      "[('10', 2)]\n",
      "language\n",
      "[('10', 1)]\n",
      "english\n",
      "[('10', 1)]\n",
      "often\n",
      "[('10', 1)]\n",
      "idea\n",
      "[('10', 1)]\n",
      "apply\n",
      "[('10', 1)]\n",
      "job\n",
      "[('10', 1)]\n",
      "college\n",
      "[('10', 1)]\n",
      "driver\n",
      "[('10', 1)]\n",
      "license\n",
      "[('10', 1)]\n",
      "major\n",
      "[('11', 1), ('15', 1), ('16', 1)]\n",
      "league\n",
      "[('11', 1)]\n",
      "baseball\n",
      "[('11', 2)]\n",
      "confirmed\n",
      "[('11', 1), ('14', 1)]\n",
      "boston\n",
      "[('11', 2)]\n",
      "red\n",
      "[('11', 3)]\n",
      "sox\n",
      "[('11', 3)]\n",
      "team\n",
      "[('11', 1)]\n",
      "watch\n",
      "[('11', 2)]\n",
      "steal\n",
      "[('11', 2)]\n",
      "hand\n",
      "[('11', 2)]\n",
      "signals\n",
      "[('11', 2)]\n",
      "york\n",
      "[('11', 3), ('20', 1)]\n",
      "yankees\n",
      "[('11', 2)]\n",
      "times\n",
      "[('11', 2), ('14', 1)]\n",
      "teams\n",
      "[('11', 1)]\n",
      "sorts\n",
      "[('11', 1)]\n",
      "reasons\n",
      "[('11', 1)]\n",
      "tell\n",
      "[('11', 1)]\n",
      "players\n",
      "[('11', 2)]\n",
      "bases\n",
      "[('11', 1)]\n",
      "pitches\n",
      "[('11', 1)]\n",
      "throw\n",
      "[('11', 2)]\n",
      "caught\n",
      "[('11', 1)]\n",
      "member\n",
      "[('11', 2)]\n",
      "training\n",
      "[('11', 1), ('15', 1)]\n",
      "looking\n",
      "[('11', 1)]\n",
      "dugout\n",
      "[('11', 1)]\n",
      "relaying\n",
      "[('11', 1)]\n",
      "message\n",
      "[('11', 1)]\n",
      "able\n",
      "[('11', 1), ('18', 2)]\n",
      "information\n",
      "[('11', 2), ('16', 1)]\n",
      "type\n",
      "[('11', 1)]\n",
      "pitch\n",
      "[('11', 1)]\n",
      "thrown\n",
      "[('11', 1)]\n",
      "pitcher\n",
      "[('11', 1)]\n",
      "fastball\n",
      "[('11', 1)]\n",
      "relayed\n",
      "[('11', 1)]\n",
      "batter\n",
      "[('11', 1)]\n",
      "socially\n",
      "[('12', 1)]\n",
      "responsible\n",
      "[('12', 4)]\n",
      "investing\n",
      "[('12', 1), ('18', 1)]\n",
      "attracting\n",
      "[('12', 1)]\n",
      "dollars\n",
      "[('12', 1)]\n",
      "fact\n",
      "[('12', 1)]\n",
      "following\n",
      "[('12', 1)]\n",
      "recent\n",
      "[('12', 1)]\n",
      "developments\n",
      "[('12', 1)]\n",
      "around\n",
      "[('12', 1), ('14', 1)]\n",
      "paris\n",
      "[('12', 1)]\n",
      "climate\n",
      "[('12', 1)]\n",
      "agreement\n",
      "[('12', 1)]\n",
      "likely\n",
      "[('12', 1), ('14', 1)]\n",
      "asking\n",
      "[('12', 1)]\n",
      "make\n",
      "[('12', 1), ('18', 1)]\n",
      "positive\n",
      "[('12', 1)]\n",
      "impact\n",
      "[('12', 2)]\n",
      "total\n",
      "[('12', 1), ('20', 1)]\n",
      "domiciled\n",
      "[('12', 1)]\n",
      "using\n",
      "[('12', 1), ('14', 1)]\n",
      "sustainable\n",
      "[('12', 3)]\n",
      "strategies\n",
      "[('12', 1)]\n",
      "grew\n",
      "[('12', 1), ('15', 1)]\n",
      "8\n",
      "[('12', 1)]\n",
      "72\n",
      "[('12', 1)]\n",
      "trillion\n",
      "[('12', 2), ('14', 2), ('20', 1)]\n",
      "6\n",
      "[('12', 1), ('13', 2)]\n",
      "57\n",
      "[('12', 1)]\n",
      "2014\n",
      "[('12', 1)]\n",
      "increase\n",
      "[('12', 1)]\n",
      "33\n",
      "[('12', 1)]\n",
      "study\n",
      "[('12', 1), ('16', 1)]\n",
      "sif\n",
      "[('12', 1)]\n",
      "forum\n",
      "[('12', 1)]\n",
      "sri\n",
      "[('12', 1)]\n",
      "grow\n",
      "[('12', 1)]\n",
      "understand\n",
      "[('12', 1), ('14', 1)]\n",
      "evaluate\n",
      "[('12', 1)]\n",
      "different\n",
      "[('12', 1), ('16', 2)]\n",
      "methods\n",
      "[('12', 1)]\n",
      "available\n",
      "[('12', 1)]\n",
      "set\n",
      "[('12', 1)]\n",
      "measurable\n",
      "[('12', 1)]\n",
      "goals\n",
      "[('12', 1)]\n",
      "build\n",
      "[('12', 1)]\n",
      "strategy\n",
      "[('12', 1), ('20', 2)]\n",
      "leaving\n",
      "[('12', 1)]\n",
      "lasting\n",
      "[('12', 1)]\n",
      "legacy\n",
      "[('12', 1)]\n",
      "yuan\n",
      "[('13', 4)]\n",
      "tear\n",
      "[('13', 1)]\n",
      "recouped\n",
      "[('13', 1)]\n",
      "losses\n",
      "[('13', 1)]\n",
      "analysts\n",
      "[('13', 1), ('20', 1)]\n",
      "still\n",
      "[('13', 1), ('18', 1)]\n",
      "room\n",
      "[('13', 1)]\n",
      "strongest\n",
      "[('13', 1)]\n",
      "changing\n",
      "[('13', 1)]\n",
      "hands\n",
      "[('13', 1)]\n",
      "5151\n",
      "[('13', 1)]\n",
      "dollar\n",
      "[('13', 1)]\n",
      "gains\n",
      "[('13', 1)]\n",
      "reuters\n",
      "[('13', 1)]\n",
      "data\n",
      "[('13', 1)]\n",
      "currency\n",
      "[('13', 2), ('20', 1)]\n",
      "appreciation\n",
      "[('13', 1)]\n",
      "quickened\n",
      "[('13', 1)]\n",
      "pace\n",
      "[('13', 1)]\n",
      "august\n",
      "[('13', 1)]\n",
      "marking\n",
      "[('13', 1)]\n",
      "best\n",
      "[('13', 1)]\n",
      "2017\n",
      "[('13', 1)]\n",
      "strengthened\n",
      "[('13', 1)]\n",
      "faster\n",
      "[('13', 1)]\n",
      "bucked\n",
      "[('13', 1)]\n",
      "trend\n",
      "[('13', 1)]\n",
      "asian\n",
      "[('13', 1)]\n",
      "currencies\n",
      "[('13', 1)]\n",
      "falling\n",
      "[('13', 1)]\n",
      "face\n",
      "[('13', 1), ('15', 1)]\n",
      "rising\n",
      "[('13', 1)]\n",
      "tensions\n",
      "[('13', 1)]\n",
      "chinese\n",
      "[('13', 1)]\n",
      "actually\n",
      "[('13', 1)]\n",
      "continued\n",
      "[('13', 1), ('17', 1)]\n",
      "appreciating\n",
      "[('13', 1)]\n",
      "experts\n",
      "[('13', 1)]\n",
      "central\n",
      "[('13', 1), ('20', 1)]\n",
      "succeeded\n",
      "[('13', 1)]\n",
      "demonstrating\n",
      "[('13', 1)]\n",
      "withstand\n",
      "[('13', 1)]\n",
      "downward\n",
      "[('13', 1)]\n",
      "pressure\n",
      "[('13', 1)]\n",
      "tightening\n",
      "[('13', 1)]\n",
      "controls\n",
      "[('13', 1)]\n",
      "foreign\n",
      "[('13', 1)]\n",
      "exchange\n",
      "[('13', 1)]\n",
      "intervention\n",
      "[('13', 1)]\n",
      "huge\n",
      "[('14', 1), ('18', 1)]\n",
      "blackhole\n",
      "[('14', 2)]\n",
      "100\n",
      "[('14', 1)]\n",
      "massive\n",
      "[('14', 2)]\n",
      "lurking\n",
      "[('14', 1)]\n",
      "toxic\n",
      "[('14', 1)]\n",
      "gas\n",
      "[('14', 2)]\n",
      "cloud\n",
      "[('14', 4)]\n",
      "near\n",
      "[('14', 1)]\n",
      "milky\n",
      "[('14', 6)]\n",
      "object\n",
      "[('14', 2)]\n",
      "rank\n",
      "[('14', 1)]\n",
      "second\n",
      "[('14', 1), ('15', 1)]\n",
      "largest\n",
      "[('14', 1)]\n",
      "supermassive\n",
      "[('14', 2)]\n",
      "sagittarius\n",
      "[('14', 2)]\n",
      "located\n",
      "[('14', 1)]\n",
      "centre\n",
      "[('14', 3)]\n",
      "galaxy\n",
      "[('14', 3)]\n",
      "astronomers\n",
      "[('14', 1)]\n",
      "keio\n",
      "[('14', 2)]\n",
      "university\n",
      "[('14', 2)]\n",
      "japan\n",
      "[('14', 1)]\n",
      "alma\n",
      "[('14', 1)]\n",
      "telescope\n",
      "[('14', 1)]\n",
      "chile\n",
      "[('14', 1)]\n",
      "observing\n",
      "[('14', 1)]\n",
      "movement\n",
      "[('14', 1)]\n",
      "gases\n",
      "[('14', 1)]\n",
      "molecules\n",
      "[('14', 1)]\n",
      "elliptical\n",
      "[('14', 1)]\n",
      "200\n",
      "[('14', 1)]\n",
      "light\n",
      "[('14', 1)]\n",
      "150\n",
      "[('14', 1)]\n",
      "kilometres\n",
      "[('14', 2)]\n",
      "wide\n",
      "[('14', 1)]\n",
      "pulled\n",
      "[('14', 1)]\n",
      "immense\n",
      "[('14', 1)]\n",
      "gravitational\n",
      "[('14', 1)]\n",
      "forces\n",
      "[('14', 1)]\n",
      "cause\n",
      "[('14', 1)]\n",
      "computer\n",
      "[('14', 1), ('16', 1)]\n",
      "models\n",
      "[('14', 1)]\n",
      "black\n",
      "[('14', 5), ('20', 1)]\n",
      "hole\n",
      "[('14', 5)]\n",
      "4\n",
      "[('14', 1)]\n",
      "detected\n",
      "[('14', 1)]\n",
      "radio\n",
      "[('14', 1)]\n",
      "waves\n",
      "[('14', 1)]\n",
      "indicated\n",
      "[('14', 1)]\n",
      "presence\n",
      "[('14', 1)]\n",
      "detection\n",
      "[('14', 1)]\n",
      "intermediate\n",
      "[('14', 1)]\n",
      "mass\n",
      "[('14', 1)]\n",
      "candidate\n",
      "[('14', 1)]\n",
      "tomoharu\n",
      "[('14', 1)]\n",
      "oka\n",
      "[('14', 3)]\n",
      "newly\n",
      "[('14', 1)]\n",
      "core\n",
      "[('14', 1)]\n",
      "old\n",
      "[('14', 1), ('15', 2)]\n",
      "dwarf\n",
      "[('14', 1)]\n",
      "cannibalised\n",
      "[('14', 1)]\n",
      "formation\n",
      "[('14', 1)]\n",
      "billions\n",
      "[('14', 1)]\n",
      "ago\n",
      "[('14', 1)]\n",
      "mr\n",
      "[('14', 2)]\n",
      "guardian\n",
      "[('14', 1)]\n",
      "time\n",
      "[('14', 1), ('15', 1), ('18', 1)]\n",
      "drawn\n",
      "[('14', 1)]\n",
      "sink\n",
      "[('14', 1)]\n",
      "published\n",
      "[('14', 1)]\n",
      "journal\n",
      "[('14', 1)]\n",
      "nature\n",
      "[('14', 1)]\n",
      "astronomy\n",
      "[('14', 1)]\n",
      "suspension\n",
      "[('15', 1)]\n",
      "drug\n",
      "[('15', 1)]\n",
      "maria\n",
      "[('15', 1)]\n",
      "sharapova\n",
      "[('15', 3)]\n",
      "returned\n",
      "[('15', 1)]\n",
      "open\n",
      "[('15', 1), ('16', 1)]\n",
      "monday\n",
      "[('15', 1)]\n",
      "night\n",
      "[('15', 1), ('17', 1)]\n",
      "30\n",
      "[('15', 1), ('18', 1)]\n",
      "memorable\n",
      "[('15', 1)]\n",
      "performance\n",
      "[('15', 1)]\n",
      "lights\n",
      "[('15', 1)]\n",
      "arthur\n",
      "[('15', 1)]\n",
      "ashe\n",
      "[('15', 1)]\n",
      "stadium\n",
      "[('15', 1)]\n",
      "downing\n",
      "[('15', 1)]\n",
      "seed\n",
      "[('15', 1)]\n",
      "simona\n",
      "[('15', 1)]\n",
      "halep\n",
      "[('15', 1)]\n",
      "today\n",
      "[('15', 1)]\n",
      "ll\n",
      "[('15', 1), ('18', 1)]\n",
      "timea\n",
      "[('15', 1)]\n",
      "babos\n",
      "[('15', 1)]\n",
      "round\n",
      "[('15', 1)]\n",
      "decade\n",
      "[('15', 1)]\n",
      "clinched\n",
      "[('15', 1)]\n",
      "grand\n",
      "[('15', 2)]\n",
      "championship\n",
      "[('15', 1)]\n",
      "age\n",
      "[('15', 2)]\n",
      "defeated\n",
      "[('15', 1)]\n",
      "serena\n",
      "[('15', 1)]\n",
      "williams\n",
      "[('15', 1)]\n",
      "wimbledon\n",
      "[('15', 1)]\n",
      "five\n",
      "[('15', 1)]\n",
      "titles\n",
      "[('15', 1)]\n",
      "sizable\n",
      "[('15', 1)]\n",
      "paycheck\n",
      "[('15', 1)]\n",
      "560\n",
      "[('15', 1)]\n",
      "500\n",
      "[('15', 1)]\n",
      "724\n",
      "[('15', 1)]\n",
      "lot\n",
      "[('15', 1)]\n",
      "teenager\n",
      "[('15', 1)]\n",
      "particularly\n",
      "[('15', 1), ('16', 1)]\n",
      "poor\n",
      "[('15', 1)]\n",
      "russian\n",
      "[('15', 1)]\n",
      "born\n",
      "[('15', 1)]\n",
      "athlete\n",
      "[('15', 1)]\n",
      "dad\n",
      "[('15', 2)]\n",
      "700\n",
      "[('15', 1)]\n",
      "nick\n",
      "[('15', 1)]\n",
      "bollettieri\n",
      "[('15', 1)]\n",
      "tennis\n",
      "[('15', 1)]\n",
      "academy\n",
      "[('15', 1)]\n",
      "slept\n",
      "[('15', 1)]\n",
      "pullout\n",
      "[('15', 1)]\n",
      "couch\n",
      "[('15', 1)]\n",
      "250\n",
      "[('15', 1)]\n",
      "apartment\n",
      "[('15', 1)]\n",
      "secured\n",
      "[('15', 1)]\n",
      "figure\n",
      "[('15', 1)]\n",
      "check\n",
      "[('15', 1)]\n",
      "headed\n",
      "[('15', 1)]\n",
      "straight\n",
      "[('15', 1)]\n",
      "tj\n",
      "[('15', 1)]\n",
      "maxx\n",
      "[('15', 1)]\n",
      "1970s\n",
      "[('16', 1)]\n",
      "1980s\n",
      "[('16', 1)]\n",
      "social\n",
      "[('16', 2)]\n",
      "informat\n",
      "[('16', 1)]\n",
      "ics\n",
      "[('16', 1)]\n",
      "focused\n",
      "[('16', 1), ('20', 1)]\n",
      "organizations\n",
      "[('16', 1)]\n",
      "sites\n",
      "[('16', 1)]\n",
      "computerization\n",
      "[('16', 1)]\n",
      "technical\n",
      "[('16', 1)]\n",
      "specialists\n",
      "[('16', 1)]\n",
      "gotten\n",
      "[('16', 1)]\n",
      "systems\n",
      "[('16', 1)]\n",
      "access\n",
      "[('16', 2), ('18', 1)]\n",
      "issues\n",
      "[('16', 1)]\n",
      "communication\n",
      "[('16', 1)]\n",
      "entertainment\n",
      "[('16', 1)]\n",
      "medical\n",
      "[('16', 1)]\n",
      "personal\n",
      "[('16', 1)]\n",
      "uses\n",
      "[('16', 1)]\n",
      "significant\n",
      "[('16', 1)]\n",
      "phenomena\n",
      "[('16', 1)]\n",
      "topics\n",
      "[('16', 1)]\n",
      "emphasize\n",
      "[('16', 1)]\n",
      "part\n",
      "[('16', 1)]\n",
      "informatics\n",
      "[('16', 1)]\n",
      "lines\n",
      "[('16', 1)]\n",
      "analysis\n",
      "[('16', 1), ('18', 1)]\n",
      "warrant\n",
      "[('16', 1)]\n",
      "serious\n",
      "[('16', 1)]\n",
      "hurricane\n",
      "[('17', 3)]\n",
      "irma\n",
      "[('17', 3)]\n",
      "forceful\n",
      "[('17', 1)]\n",
      "atlantic\n",
      "[('17', 1)]\n",
      "storms\n",
      "[('17', 1)]\n",
      "century\n",
      "[('17', 1)]\n",
      "churned\n",
      "[('17', 1)]\n",
      "ocean\n",
      "[('17', 1)]\n",
      "collision\n",
      "[('17', 1)]\n",
      "course\n",
      "[('17', 1)]\n",
      "puerto\n",
      "[('17', 2)]\n",
      "rico\n",
      "[('17', 2)]\n",
      "virgin\n",
      "[('17', 1)]\n",
      "islands\n",
      "[('17', 2)]\n",
      "bearing\n",
      "[('17', 1)]\n",
      "northern\n",
      "[('17', 2)]\n",
      "caribbean\n",
      "[('17', 1)]\n",
      "devastating\n",
      "[('17', 1)]\n",
      "mix\n",
      "[('17', 1)]\n",
      "fierce\n",
      "[('17', 1)]\n",
      "winds\n",
      "[('17', 2)]\n",
      "surf\n",
      "[('17', 1)]\n",
      "rain\n",
      "[('17', 1)]\n",
      "eye\n",
      "[('17', 1)]\n",
      "category\n",
      "[('17', 1)]\n",
      "storm\n",
      "[('17', 2)]\n",
      "packing\n",
      "[('17', 1)]\n",
      "185\n",
      "[('17', 1)]\n",
      "miles\n",
      "[('17', 1)]\n",
      "per\n",
      "[('17', 2)]\n",
      "hour\n",
      "[('17', 2)]\n",
      "295\n",
      "[('17', 1)]\n",
      "km\n",
      "[('17', 1)]\n",
      "sweep\n",
      "[('17', 1)]\n",
      "leeward\n",
      "[('17', 1)]\n",
      "east\n",
      "[('17', 1)]\n",
      "wednesday\n",
      "[('17', 1)]\n",
      "en\n",
      "[('17', 1)]\n",
      "route\n",
      "[('17', 1)]\n",
      "florida\n",
      "[('17', 1)]\n",
      "landfall\n",
      "[('17', 1)]\n",
      "saturday\n",
      "[('17', 1)]\n",
      "center\n",
      "[('17', 1)]\n",
      "nhc\n",
      "[('17', 2)]\n",
      "miami\n",
      "[('17', 1)]\n",
      "reported\n",
      "[('17', 1)]\n",
      "threat\n",
      "[('17', 1)]\n",
      "posed\n",
      "[('17', 1)]\n",
      "mainland\n",
      "[('17', 1)]\n",
      "described\n",
      "[('17', 1)]\n",
      "forecasters\n",
      "[('17', 1)]\n",
      "potentially\n",
      "[('17', 1)]\n",
      "catastrophic\n",
      "[('17', 1)]\n",
      "loomed\n",
      "[('17', 1)]\n",
      "texas\n",
      "[('17', 1)]\n",
      "louisiana\n",
      "[('17', 1)]\n",
      "reel\n",
      "[('17', 1)]\n",
      "widespread\n",
      "[('17', 1)]\n",
      "destructive\n",
      "[('17', 1)]\n",
      "flooding\n",
      "[('17', 1)]\n",
      "harvey\n",
      "[('17', 1)]\n",
      "jim\n",
      "[('18', 1)]\n",
      "cramer\n",
      "[('18', 7)]\n",
      "always\n",
      "[('18', 1)]\n",
      "bull\n",
      "[('18', 1)]\n",
      "somewhere\n",
      "[('18', 1)]\n",
      "toughest\n",
      "[('18', 1)]\n",
      "find\n",
      "[('18', 1)]\n",
      "pockets\n",
      "[('18', 1)]\n",
      "opportunity\n",
      "[('18', 1)]\n",
      "became\n",
      "[('18', 1)]\n",
      "successful\n",
      "[('18', 1)]\n",
      "managers\n",
      "[('18', 1)]\n",
      "14\n",
      "[('18', 1)]\n",
      "hedge\n",
      "[('18', 1)]\n",
      "fund\n",
      "[('18', 1)]\n",
      "berkowitz\n",
      "[('18', 1)]\n",
      "generated\n",
      "[('18', 1)]\n",
      "average\n",
      "[('18', 1)]\n",
      "annual\n",
      "[('18', 1), ('20', 1)]\n",
      "return\n",
      "[('18', 1)]\n",
      "24\n",
      "[('18', 1)]\n",
      "individual\n",
      "[('18', 1)]\n",
      "success\n",
      "[('18', 1), ('20', 1)]\n",
      "holding\n",
      "[('18', 1)]\n",
      "secrets\n",
      "[('18', 1)]\n",
      "rather\n",
      "[('18', 1)]\n",
      "limiting\n",
      "[('18', 1)]\n",
      "skills\n",
      "[('18', 1)]\n",
      "benefit\n",
      "[('18', 2)]\n",
      "granting\n",
      "[('18', 1)]\n",
      "subscriber\n",
      "[('18', 1)]\n",
      "portfolio\n",
      "[('18', 1)]\n",
      "membership\n",
      "[('18', 1)]\n",
      "club\n",
      "[('18', 1)]\n",
      "action\n",
      "[('18', 1)]\n",
      "alerts\n",
      "[('18', 2)]\n",
      "plus\n",
      "[('18', 1)]\n",
      "buy\n",
      "[('18', 1)]\n",
      "price\n",
      "[('18', 2)]\n",
      "current\n",
      "[('18', 1)]\n",
      "returns\n",
      "[('18', 1)]\n",
      "holdings\n",
      "[('18', 1)]\n",
      "immediately\n",
      "[('18', 1)]\n",
      "upon\n",
      "[('18', 1)]\n",
      "signup\n",
      "[('18', 1)]\n",
      "behind\n",
      "[('18', 1)]\n",
      "decisions\n",
      "[('18', 1)]\n",
      "arguably\n",
      "[('18', 1)]\n",
      "valuable\n",
      "[('18', 1)]\n",
      "offerings\n",
      "[('18', 1)]\n",
      "trade\n",
      "[('18', 2)]\n",
      "emails\n",
      "[('18', 1)]\n",
      "right\n",
      "[('18', 1)]\n",
      "makes\n",
      "[('18', 1)]\n",
      "allows\n",
      "[('18', 1)]\n",
      "invest\n",
      "[('18', 2)]\n",
      "likewell\n",
      "[('18', 1)]\n",
      "isgetting\n",
      "[('18', 1)]\n",
      "skilled\n",
      "[('18', 1)]\n",
      "manager\n",
      "[('18', 2)]\n",
      "insight\n",
      "[('18', 1)]\n",
      "maintaining\n",
      "[('18', 1)]\n",
      "control\n",
      "[('18', 1)]\n",
      "please\n",
      "[('18', 1)]\n",
      "walks\n",
      "[('18', 1)]\n",
      "everything\n",
      "[('18', 1)]\n",
      "moves\n",
      "[('18', 1)]\n",
      "professional\n",
      "[('18', 1)]\n",
      "without\n",
      "[('18', 1)]\n",
      "spending\n",
      "[('18', 1)]\n",
      "india\n",
      "[('20', 2)]\n",
      "demonetized\n",
      "[('20', 1)]\n",
      "far\n",
      "[('20', 1)]\n",
      "tarnishing\n",
      "[('20', 1)]\n",
      "prime\n",
      "[('20', 2)]\n",
      "minister\n",
      "[('20', 2)]\n",
      "narendra\n",
      "[('20', 2)]\n",
      "modi\n",
      "[('20', 2)]\n",
      "image\n",
      "[('20', 1)]\n",
      "ultimately\n",
      "[('20', 1)]\n",
      "viewed\n",
      "[('20', 1)]\n",
      "remember\n",
      "[('20', 1)]\n",
      "demonetization\n",
      "[('20', 2)]\n",
      "intended\n",
      "[('20', 1)]\n",
      "economic\n",
      "[('20', 1)]\n",
      "south\n",
      "[('20', 1)]\n",
      "asia\n",
      "[('20', 1)]\n",
      "eurasia\n",
      "[('20', 1)]\n",
      "group\n",
      "[('20', 1)]\n",
      "sasha\n",
      "[('20', 1)]\n",
      "riser\n",
      "[('20', 1)]\n",
      "kositsky\n",
      "[('20', 1)]\n",
      "drama\n",
      "[('20', 1)]\n",
      "allowed\n",
      "[('20', 1)]\n",
      "demonstrate\n",
      "[('20', 1)]\n",
      "visible\n",
      "[('20', 1)]\n",
      "commitment\n",
      "[('20', 1)]\n",
      "fighting\n",
      "[('20', 1)]\n",
      "corruption\n",
      "[('20', 1)]\n",
      "earlier\n",
      "[('20', 1)]\n",
      "reserve\n",
      "[('20', 1)]\n",
      "rbi\n",
      "[('20', 1)]\n",
      "28\n",
      "[('20', 1)]\n",
      "rupees\n",
      "[('20', 1)]\n",
      "239\n",
      "[('20', 1)]\n",
      "worth\n",
      "[('20', 1)]\n",
      "cancelled\n",
      "[('20', 1)]\n",
      "value\n",
      "[('20', 1)]\n",
      "notes\n",
      "[('20', 1)]\n",
      "deposited\n",
      "[('20', 1)]\n",
      "exchanged\n",
      "[('20', 1)]\n",
      "implemented\n",
      "[('20', 1)]\n",
      "shy\n",
      "[('20', 1)]\n",
      "number\n",
      "[('20', 1)]\n",
      "circulation\n",
      "[('20', 1)]\n",
      "plans\n",
      "[('20', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Lets understand the logic\n",
    "\n",
    "\n",
    "word_index = {}  # We are creating an empty dictionary to store the word frequency index.\n",
    "for file_name, words in final_words.items(): # Loop through each document and the words in it\n",
    "    # 'file_name' is the name of the document, and 'words' is the list of words in that document.\n",
    "    \n",
    "    for word in words: # We loop through each word in the current document's word list.   \n",
    "        # If this word is not already in our index, we add it and set its value to an empty dictionary.\n",
    "        if word not in word_index:\n",
    "            word_index[word] = {}\n",
    "        \n",
    "        # If this document hasn't been seen for this word, we add it to the word's dictionary with a count of 0.\n",
    "        if file_name not in word_index[word]:\n",
    "            word_index[word][file_name] = 0\n",
    "        \n",
    "        # Now, we increment the count of this word for this specific document.\n",
    "        word_index[word][file_name] += 1\n",
    "\n",
    "# Output the word index in a readable format\n",
    "for word, doc_freqs in word_index.items():\n",
    "    # For each word in the word_index, we get the dictionary of document frequencies (doc_freqs).\n",
    "    \n",
    "    # We create a list of tuples, where each tuple contains the document name (without extension) and the frequency of the word in that document.\n",
    "    doc_list = [(doc.split('.')[0], freq) for doc, freq in doc_freqs.items()]\n",
    "    \n",
    "    # Print the word and the list of documents where it appears along with the frequency.\n",
    "    print(f\"{word}\\n{doc_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fire a Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # Step 1: Define a set of words as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query words: ['my', 'name', 'is', 'swaraj,', 'i', 'am', 'from', 'nasa,', 'i', 'love', 'planet', 'krishna,', 'plant', 'circulation', 'burn', 'days']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = \"My name is Swaraj, I am from Nasa, I love planet Krishna, plant circulation burn days \"  # Example\n",
    "query = query.lower()  # Convert input to lowercase\n",
    "query_words = query.split()  # Split the input into individual words\n",
    "print(f\"Query words: {query_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # Step 2: Create a list with words from each text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'my' is not found in the index.\n",
      "'name' is not found in the index.\n",
      "'is' is not found in the index.\n",
      "'swaraj,' is not found in the index.\n",
      "'i' is not found in the index.\n",
      "'am' is not found in the index.\n",
      "'from' is not found in the index.\n",
      "'nasa,' is not found in the index.\n",
      "'i' is not found in the index.\n",
      "'love' is found in the index.\n",
      "'planet' is found in the index.\n",
      "'krishna,' is not found in the index.\n",
      "'plant' is not found in the index.\n",
      "'circulation' is found in the index.\n",
      "'burn' is found in the index.\n",
      "'days' is found in the index.\n"
     ]
    }
   ],
   "source": [
    "for word in query_words:\n",
    "    if word in word_index:\n",
    "        print(f\"'{word}' is found in the index.\")\n",
    "    else:\n",
    "        print(f\"'{word}' is not found in the index.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # Load stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered query words (after removing stopwords): ['name', 'swaraj,', 'nasa,', 'love', 'planet', 'krishna,', 'plant', 'circulation', 'burn', 'days']\n"
     ]
    }
   ],
   "source": [
    "def load_stopwords(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        return set(file.read().split())\n",
    "stopwords = load_stopwords('/Volumes/Jagannath/Fall 2024/Cloud Computing/Assignment_1_data/stopwords.txt') \n",
    "\n",
    "# Step 3: Remove stop words from the query\n",
    "filtered_query = [word for word in query_words if word not in stopwords]\n",
    "print(f\"Filtered query words (after removing stopwords): {filtered_query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # Step 4: Score each document by summing the frequency of each word in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document scores before sorting:\n",
      "06.txt.txt: 1\n",
      "01.txt.txt: 4\n",
      "09.txt.txt: 1\n",
      "20.txt.txt: 1\n"
     ]
    }
   ],
   "source": [
    "doc_scores = {}\n",
    "\n",
    "# Loop through each word in the filtered query and sum the frequencies in the documents\n",
    "for word in filtered_query:\n",
    "    if word in word_index:\n",
    "        for doc, freq in word_index[word].items():\n",
    "            if doc not in doc_scores:\n",
    "                doc_scores[doc] = 0  # Initialize the score for the document\n",
    "            doc_scores[doc] += freq  # Add the frequency of the word to the document's score\n",
    "\n",
    "# Print the document scores to verify\n",
    "print(\"Document scores before sorting:\")\n",
    "for doc, score in doc_scores.items():\n",
    "    print(f\"{doc}: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # Step 5: Print pairwise document IDs and scores, sorted in descending order of score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Documents and their scores by total frequency:\n",
      "01.txt.txt: 4\n",
      "06.txt.txt: 1\n",
      "09.txt.txt: 1\n",
      "20.txt.txt: 1\n"
     ]
    }
   ],
   "source": [
    "sorted_docs = sorted([(doc, score) for doc, score in doc_scores.items() if score > 0], key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Output the result\n",
    "print(\"\\nDocuments and their scores by total frequency:\")\n",
    "for doc, score in sorted_docs:\n",
    "    print(f\"{doc}: {score}\")\n",
    "\n",
    "if not sorted_docs:\n",
    "    print(\"No documents matched.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Lambda Function \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We are solving the same problem using lambda function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read each text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of .txt files:\n",
      "01.txt.txt\n",
      "02.txt.txt\n",
      "03.txt.txt\n",
      "04.txt.txt\n",
      "05.txt.txt\n",
      "06.txt.txt\n",
      "07.txt.txt\n",
      "08.txt.txt\n",
      "09.txt.txt\n",
      "10.txt.txt\n",
      "11.txt.txt\n",
      "12.txt.txt\n",
      "13.txt.txt\n",
      "14.txt.txt\n",
      "15.txt.txt\n",
      "16.txt.txt\n",
      "17.txt.txt\n",
      "18.txt.txt\n",
      "19.txt.txt\n",
      "20.txt.txt\n"
     ]
    }
   ],
   "source": [
    "import glob  \n",
    "import os    \n",
    "folder_path = '/Volumes/Jagannath/Fall 2024/Cloud Computing/Assignment_1_data/Assignment1 txt files' # File path\n",
    "\n",
    "# The lambda function takes the full file path and extracts only the base file name (the name without the folder path)\n",
    "file_names = sorted(map(lambda f: os.path.basename(f), glob.glob(os.path.join(folder_path, '*.txt'))))\n",
    "\n",
    "# Printing the list of file names in a sorted order. The *file_names unpacks the list of files into the print statement\n",
    "print(\"List of .txt files:\", *file_names, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After a 15-month suspension for drug use, Maria Sharapova returned to the U.S. Open for the first time in three years on Monday night. The 30-year-old put on a memorable performance under the lights of Arthur Ashe Stadium, downing No. 2 seed Simona Halep. Today, she'll face Timea Babos in the second round of the major.\n",
      "It's been over a decade since Sharapova clinched her first Grand Slam championship. At age 17, she defeated Serena Williams in the 2004 Wimbledon final. It was her first of what would be five Grand Slam titles, and it came with a sizable paycheck: 560,500, or about $724,000.\n",
      "That's a lot of money for any teenager, particularly for one who grew up poor: At age seven, the Russian-born athlete and her dad arrived in the U.S. with just $700. While training at Nick Bollettieri's Tennis Academy, she slept on a pullout couch next to her dad in a $250-a-month apartment.\n",
      "\n",
      "And, when 17-year-old Sharapova secured her six-figure check, she headed straight to TJ Maxx.\n"
     ]
    }
   ],
   "source": [
    "file_to_open = os.path.join(folder_path, '15.txt.txt')  # Construct the full path for the file\n",
    "\n",
    "# Using a lambda function to open the file and read the first 1000 characters\n",
    "read_first_1000 = lambda fp: open(fp, 'r', encoding='utf-8', errors='ignore').read()[:1000]\n",
    "\n",
    "print(read_first_1000(file_to_open)) # Call the lambda function and print the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert each words in the file to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01.txt.txt (lowercased): in 10 days, nasa's cassini spacecraft will nose-dive into saturn and burn up in the planet's atmosphere. it's the final, suicidal step of a months-long dance through saturn's rings that has given scie\n",
      "--------------------------------------------------\n",
      "02.txt.txt (lowercased): microsoft responded strongly to the trump administration's decision on tuesday to move toward rescinding or replacing daca within six months.\n",
      "\"there is nothing that we will be pushing on more strongly\n",
      "--------------------------------------------------\n",
      "03.txt.txt (lowercased): putin said that the development of ai raises both \"colossal opportunities\" and \"threats that are difficult to predict.\"\n",
      "\"whoever becomes the leader in this sphere will become the ruler of the world,\" \n",
      "--------------------------------------------------\n",
      "04.txt.txt (lowercased): meat that doesn't require the slaughter of animals. fashion that provides an eco-friendly alternative to leather. dairy products that are lactose-free and brewed in a lab.\n",
      "most san franciscans aren't \n",
      "--------------------------------------------------\n",
      "05.txt.txt (lowercased): so, first of all, i just want to say that in my view the equity analogy is incomplete. especially if doing a blockchain or token in the u.s., to be compliant with all regulations you need it to have a\n",
      "--------------------------------------------------\n",
      "06.txt.txt (lowercased): former us president barack obama decried his successor's decision to end an amnesty for 800,000 people brought to america illegally as children, describing it as \"wrong,\" \"self-defeating\" and \"cruel.\"\n",
      "--------------------------------------------------\n",
      "07.txt.txt (lowercased): bond yields are due to rise in a big way, according to strategist larry mcdonald of acg analytics.\n",
      "on tuesday, the 10-year u.s. treasury yield fell to 2.07 percent  its lowest level since november 201\n",
      "--------------------------------------------------\n",
      "08.txt.txt (lowercased): apple's next iphone cycle is expected to be one of the biggest yet  but that doesn't necessarily mean the stock will keep soaring, according to one of the company's most well-known followers.\n",
      "\n",
      "there c\n",
      "--------------------------------------------------\n",
      "09.txt.txt (lowercased): cassini launched on its billion-mile journey from earth to saturn on oct. 15, 1997. it was named for the astronomer giovanni cassini, who discovered four of the planet's moons and a gap in its rings. \n",
      "--------------------------------------------------\n",
      "10.txt.txt (lowercased): former president barack obama sharply criticized the trump administration's decision to end the daca program that protects young undocumented migrants from deportation, slamming the white house's move\n",
      "--------------------------------------------------\n",
      "11.txt.txt (lowercased): major league baseball confirmed that the boston red sox baseball team used an apple watch to steal hand signals from the new york yankees, according to the new york times.\n",
      "hand signals are used by tea\n",
      "--------------------------------------------------\n",
      "12.txt.txt (lowercased): socially responsible investing is attracting more dollars than ever. in fact, following the recent developments around the paris climate agreement, many investors are likely asking themselves how they\n",
      "--------------------------------------------------\n",
      "13.txt.txt (lowercased): china's yuan has been on a tear all year and has now recouped last year's losses  and analysts say there's still room to run.\n",
      "at its strongest level on tuesday, the yuan was changing hands at 6.5151 a\n",
      "--------------------------------------------------\n",
      "14.txt.txt (lowercased): a huge blackhole - about 100,000 times more massive than our sun - has been discovered lurking in a toxic gas cloud near the heart of the milky way.\n",
      "\n",
      "if confirmed, the object will rank as the second l\n",
      "--------------------------------------------------\n",
      "15.txt.txt (lowercased): after a 15-month suspension for drug use, maria sharapova returned to the u.s. open for the first time in three years on monday night. the 30-year-old put on a memorable performance under the lights o\n",
      "--------------------------------------------------\n",
      "16.txt.txt (lowercased): through the 1970s and 1980s, much of the social informat-\n",
      "ics research focused on organizations because they were\n",
      "the major sites of computerization. it is only in the last few\n",
      "years that many people \n",
      "--------------------------------------------------\n",
      "17.txt.txt (lowercased): hurricane irma, one of the most forceful atlantic storms in a century, churned across the ocean on tuesday on a collision course with puerto rico and the virgin islands, bearing down on the northern c\n",
      "--------------------------------------------------\n",
      "18.txt.txt (lowercased): according to jim cramer, \"there's always a bull market somewhere.\" even in the toughest markets, cramer's been able to find financial pockets of opportunity. he became well-known as one of the most su\n",
      "--------------------------------------------------\n",
      "19.txt.txt (lowercased): microsoft responded strongly to the trump administration's decision on tuesday to move toward rescinding or replacing daca within six months.\n",
      "\"there is nothing that we will be pushing on more strongly\n",
      "--------------------------------------------------\n",
      "20.txt.txt (lowercased): india's demonetized currency may have found its way back into the system but analysts suggest that far from tarnishing prime minister narendra modi's image, the strategy will ultimately be viewed as a\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for file_name in file_names:\n",
    "    file_path = os.path.join(folder_path, file_name)  # Generate the full path for each file by combining the folder path and file name.\n",
    "    # Using a lambda function to open and read the content, converting it to lowercase\n",
    "    read_file_lowercase = lambda fp: open(fp, 'r', encoding='utf-8', errors='ignore').read().lower()\n",
    "    content = read_file_lowercase(file_path)\n",
    "    print(f\"{file_name} (lowercased): {content[:200]}\\n{'-'*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a list with words from each text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01.txt.txt: ['in', '10', 'days', 'nasa', 's', 'cassini', 'spacecraft', 'will', 'nose', 'dive']\n",
      "02.txt.txt: ['microsoft', 'responded', 'strongly', 'to', 'the', 'trump', 'administration', 's', 'decision', 'on']\n",
      "03.txt.txt: ['putin', 'said', 'that', 'the', 'development', 'of', 'ai', 'raises', 'both', 'colossal']\n",
      "04.txt.txt: ['meat', 'that', 'doesn', 't', 'require', 'the', 'slaughter', 'of', 'animals', 'fashion']\n",
      "05.txt.txt: ['so', 'first', 'of', 'all', 'i', 'just', 'want', 'to', 'say', 'that']\n",
      "06.txt.txt: ['former', 'us', 'president', 'barack', 'obama', 'decried', 'his', 'successor', 's', 'decision']\n",
      "07.txt.txt: ['bond', 'yields', 'are', 'due', 'to', 'rise', 'in', 'a', 'big', 'way']\n",
      "08.txt.txt: ['apple', 's', 'next', 'iphone', 'cycle', 'is', 'expected', 'to', 'be', 'one']\n",
      "09.txt.txt: ['cassini', 'launched', 'on', 'its', 'billion', 'mile', 'journey', 'from', 'earth', 'to']\n",
      "10.txt.txt: ['former', 'president', 'barack', 'obama', 'sharply', 'criticized', 'the', 'trump', 'administration', 's']\n",
      "11.txt.txt: ['major', 'league', 'baseball', 'confirmed', 'that', 'the', 'boston', 'red', 'sox', 'baseball']\n",
      "12.txt.txt: ['socially', 'responsible', 'investing', 'is', 'attracting', 'more', 'dollars', 'than', 'ever', 'in']\n",
      "13.txt.txt: ['china', 's', 'yuan', 'has', 'been', 'on', 'a', 'tear', 'all', 'year']\n",
      "14.txt.txt: ['a', 'huge', 'blackhole', 'about', '100', '000', 'times', 'more', 'massive', 'than']\n",
      "15.txt.txt: ['after', 'a', '15', 'month', 'suspension', 'for', 'drug', 'use', 'maria', 'sharapova']\n",
      "16.txt.txt: ['through', 'the', '1970s', 'and', '1980s', 'much', 'of', 'the', 'social', 'informat']\n",
      "17.txt.txt: ['hurricane', 'irma', 'one', 'of', 'the', 'most', 'forceful', 'atlantic', 'storms', 'in']\n",
      "18.txt.txt: ['according', 'to', 'jim', 'cramer', 'there', 's', 'always', 'a', 'bull', 'market']\n",
      "19.txt.txt: ['microsoft', 'responded', 'strongly', 'to', 'the', 'trump', 'administration', 's', 'decision', 'on']\n",
      "20.txt.txt: ['india', 's', 'demonetized', 'currency', 'may', 'have', 'found', 'its', 'way', 'back']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "# Lambda function to split content into words by removing punctuation\n",
    "split_words = lambda content: re.findall(r'\\b\\w+\\b', content)\n",
    "\n",
    "# Lambda function to read the file, convert content to lowercase, and split into words\n",
    "process_file = lambda file_path: split_words(open(file_path, 'r', encoding='utf-8', errors='ignore').read().lower())\n",
    "\n",
    "all_words = {}\n",
    "# Loop through file names and process each file using the lambda function\n",
    "for file_name in file_names:\n",
    "    file_path = os.path.join(folder_path, file_name)  # Generate full path for the file\n",
    "    all_words[file_name] = process_file(file_path)  # Process and store words for each file\n",
    "for file_name, words in all_words.items():\n",
    "    print(f\"{file_name}: {words[:10]}\")  # Print the first 10 words as a sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample stopwords: [\"isn't\", 'be', 'herself', 'do', 'a', 'your', \"hasn't\", 'she', 'further', 'the', 'and', 'being', 'why', 'same', 'because', \"it's\", \"wouldn't\", \"haven't\", 'his', 'more']\n"
     ]
    }
   ],
   "source": [
    "# Lambda function to load stopwords by reading the file and splitting its content into a set\n",
    "load_stopwords = lambda filepath: set(open(filepath, 'r').read().split())\n",
    "stopwords_path = '/Volumes/Jagannath/Fall 2024/Cloud Computing/Assignment_1_data/stopwords.txt'\n",
    "stopwords = load_stopwords(stopwords_path)\n",
    "print(\"Sample stopwords:\", list(stopwords)[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove stop words from each list and get the final list of words for each text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01.txt.txt: ['10', 'days', 'nasa', 's', 'cassini', 'spacecraft', 'will', 'nose', 'dive', 'saturn']\n",
      "02.txt.txt: ['microsoft', 'responded', 'strongly', 'trump', 'administration', 's', 'decision', 'tuesday', 'move', 'toward']\n",
      "03.txt.txt: ['putin', 'said', 'development', 'ai', 'raises', 'colossal', 'opportunities', 'threats', 'difficult', 'predict']\n",
      "04.txt.txt: ['meat', 'doesn', 't', 'require', 'slaughter', 'animals', 'fashion', 'provides', 'eco', 'friendly']\n",
      "05.txt.txt: ['first', 'just', 'want', 'say', 'view', 'equity', 'analogy', 'incomplete', 'especially', 'blockchain']\n",
      "06.txt.txt: ['former', 'us', 'president', 'barack', 'obama', 'decried', 'successor', 's', 'decision', 'end']\n",
      "07.txt.txt: ['bond', 'yields', 'due', 'rise', 'big', 'way', 'according', 'strategist', 'larry', 'mcdonald']\n",
      "08.txt.txt: ['apple', 's', 'next', 'iphone', 'cycle', 'expected', 'one', 'biggest', 'yet', 'doesn']\n",
      "09.txt.txt: ['cassini', 'launched', 'billion', 'mile', 'journey', 'earth', 'saturn', 'oct', '15', '1997']\n",
      "10.txt.txt: ['former', 'president', 'barack', 'obama', 'sharply', 'criticized', 'trump', 'administration', 's', 'decision']\n",
      "11.txt.txt: ['major', 'league', 'baseball', 'confirmed', 'boston', 'red', 'sox', 'baseball', 'team', 'used']\n",
      "12.txt.txt: ['socially', 'responsible', 'investing', 'attracting', 'dollars', 'ever', 'fact', 'following', 'recent', 'developments']\n",
      "13.txt.txt: ['china', 's', 'yuan', 'tear', 'year', 'now', 'recouped', 'last', 'year', 's']\n",
      "14.txt.txt: ['huge', 'blackhole', '100', '000', 'times', 'massive', 'sun', 'discovered', 'lurking', 'toxic']\n",
      "15.txt.txt: ['15', 'month', 'suspension', 'drug', 'use', 'maria', 'sharapova', 'returned', 'u', 's']\n",
      "16.txt.txt: ['1970s', '1980s', 'much', 'social', 'informat', 'ics', 'research', 'focused', 'organizations', 'major']\n",
      "17.txt.txt: ['hurricane', 'irma', 'one', 'forceful', 'atlantic', 'storms', 'century', 'churned', 'across', 'ocean']\n",
      "18.txt.txt: ['according', 'jim', 'cramer', 's', 'always', 'bull', 'market', 'somewhere', 'even', 'toughest']\n",
      "19.txt.txt: ['microsoft', 'responded', 'strongly', 'trump', 'administration', 's', 'decision', 'tuesday', 'move', 'toward']\n",
      "20.txt.txt: ['india', 's', 'demonetized', 'currency', 'may', 'found', 'way', 'back', 'system', 'analysts']\n"
     ]
    }
   ],
   "source": [
    "final_words = {}  # Dictionary to store the final list of words (after removing stopwords)\n",
    "\n",
    "# Lambda function to filter out stopwords from the word list\n",
    "filter_stopwords = lambda words, stopwords: list(filter(lambda word: word not in stopwords, words))\n",
    "\n",
    "# Remove stopwords from each file's word list using the lambda function\n",
    "for file_name, words in all_words.items():\n",
    "    final_words[file_name] = filter_stopwords(words, stopwords)  # Apply the lambda function to filter words\n",
    "for file_name, words in final_words.items():\n",
    "    print(f\"{file_name}: {words[:10]}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build dictionary for each word with KEY being the document ID (file name) and VALUE as frequency (number of times the word appears in that particular file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "[('01', 1), ('07', 1), ('08', 1), ('20', 1)]\n",
      "days\n",
      "[('01', 1)]\n",
      "nasa\n",
      "[('01', 1)]\n",
      "s\n",
      "[('01', 5), ('02', 2), ('03', 2), ('04', 3), ('05', 4), ('06', 3), ('07', 3), ('08', 5), ('09', 4), ('10', 4), ('12', 1), ('13', 7), ('15', 5), ('17', 2), ('18', 6), ('19', 2), ('20', 4)]\n",
      "cassini\n",
      "[('01', 2), ('09', 5)]\n",
      "spacecraft\n",
      "[('01', 1)]\n",
      "will\n",
      "[('01', 1), ('02', 1), ('03', 2), ('07', 1), ('08', 1), ('14', 2), ('16', 1), ('19', 1), ('20', 1)]\n",
      "nose\n",
      "[('01', 1)]\n",
      "dive\n",
      "[('01', 1)]\n",
      "saturn\n",
      "[('01', 3), ('09', 3)]\n",
      "burn\n",
      "[('01', 1)]\n",
      "planet\n",
      "[('01', 2), ('09', 1)]\n",
      "atmosphere\n",
      "[('01', 1)]\n",
      "final\n",
      "[('01', 1), ('15', 1)]\n",
      "suicidal\n",
      "[('01', 1)]\n",
      "step\n",
      "[('01', 1), ('06', 1)]\n",
      "months\n",
      "[('01', 1), ('02', 1), ('08', 2), ('09', 1), ('19', 1), ('20', 1)]\n",
      "long\n",
      "[('01', 1)]\n",
      "dance\n",
      "[('01', 1)]\n",
      "rings\n",
      "[('01', 1), ('09', 1)]\n",
      "given\n",
      "[('01', 1), ('07', 1)]\n",
      "scientists\n",
      "[('01', 1), ('04', 1), ('14', 1)]\n",
      "unprecedented\n",
      "[('01', 1)]\n",
      "view\n",
      "[('01', 1), ('05', 1), ('07', 1)]\n",
      "sixth\n",
      "[('01', 1)]\n",
      "sun\n",
      "[('01', 1), ('14', 1)]\n",
      "also\n",
      "[('01', 1), ('09', 1), ('14', 1)]\n",
      "end\n",
      "[('01', 2), ('06', 1), ('10', 1)]\n",
      "mission\n",
      "[('01', 1)]\n",
      "revolutionized\n",
      "[('01', 1)]\n",
      "understanding\n",
      "[('01', 1), ('16', 1)]\n",
      "opened\n",
      "[('01', 1)]\n",
      "eyes\n",
      "[('01', 1)]\n",
      "two\n",
      "[('01', 1)]\n",
      "worlds\n",
      "[('01', 1)]\n",
      "home\n",
      "[('01', 1), ('10', 1), ('16', 3)]\n",
      "alien\n",
      "[('01', 1)]\n",
      "life\n",
      "[('01', 1)]\n",
      "moons\n",
      "[('01', 1), ('09', 1)]\n",
      "titan\n",
      "[('01', 1), ('09', 2)]\n",
      "enceladus\n",
      "[('01', 1)]\n",
      "really\n",
      "[('01', 1)]\n",
      "era\n",
      "[('01', 1), ('16', 1)]\n",
      "fans\n",
      "[('01', 1)]\n",
      "devastated\n",
      "[('01', 1)]\n",
      "microsoft\n",
      "[('02', 4), ('03', 1), ('19', 4)]\n",
      "responded\n",
      "[('02', 1), ('19', 1)]\n",
      "strongly\n",
      "[('02', 2), ('19', 2)]\n",
      "trump\n",
      "[('02', 1), ('06', 2), ('10', 1), ('19', 1)]\n",
      "administration\n",
      "[('02', 1), ('10', 2), ('19', 1)]\n",
      "decision\n",
      "[('02', 1), ('06', 2), ('10', 3), ('19', 1)]\n",
      "tuesday\n",
      "[('02', 1), ('07', 2), ('08', 1), ('13', 1), ('17', 2), ('19', 1)]\n",
      "move\n",
      "[('02', 2), ('06', 1), ('07', 1), ('10', 1), ('19', 2), ('20', 2)]\n",
      "toward\n",
      "[('02', 1), ('19', 1)]\n",
      "rescinding\n",
      "[('02', 1), ('19', 1)]\n",
      "replacing\n",
      "[('02', 1), ('19', 1)]\n",
      "daca\n",
      "[('02', 1), ('06', 1), ('10', 1), ('19', 1)]\n",
      "within\n",
      "[('02', 1), ('19', 1)]\n",
      "six\n",
      "[('02', 1), ('15', 1), ('19', 1)]\n",
      "nothing\n",
      "[('02', 1), ('06', 1), ('19', 1)]\n",
      "pushing\n",
      "[('02', 1), ('13', 1), ('19', 1)]\n",
      "congress\n",
      "[('02', 1), ('06', 1), ('10', 1), ('19', 1)]\n",
      "act\n",
      "[('02', 1), ('10', 1), ('19', 1)]\n",
      "president\n",
      "[('02', 1), ('06', 1), ('10', 2), ('19', 1)]\n",
      "chief\n",
      "[('02', 1), ('19', 1)]\n",
      "legal\n",
      "[('02', 1), ('10', 1), ('19', 1)]\n",
      "officer\n",
      "[('02', 1), ('19', 1)]\n",
      "brad\n",
      "[('02', 1), ('19', 1)]\n",
      "smith\n",
      "[('02', 5), ('19', 5)]\n",
      "said\n",
      "[('02', 3), ('03', 3), ('05', 1), ('06', 1), ('07', 1), ('08', 1), ('10', 1), ('14', 2), ('19', 3)]\n",
      "interview\n",
      "[('02', 1), ('05', 1), ('19', 1)]\n",
      "npr\n",
      "[('02', 1), ('19', 1)]\n",
      "put\n",
      "[('02', 1), ('15', 1), ('19', 1)]\n",
      "stake\n",
      "[('02', 1), ('19', 1)]\n",
      "ground\n",
      "[('02', 1), ('19', 1)]\n",
      "care\n",
      "[('02', 1), ('19', 1)]\n",
      "tax\n",
      "[('02', 1), ('19', 1)]\n",
      "reform\n",
      "[('02', 1), ('19', 1)]\n",
      "bill\n",
      "[('02', 1), ('04', 1), ('19', 1)]\n",
      "noting\n",
      "[('02', 1), ('19', 1)]\n",
      "entire\n",
      "[('02', 1), ('18', 1), ('19', 1)]\n",
      "business\n",
      "[('02', 1), ('19', 1)]\n",
      "community\n",
      "[('02', 1), ('19', 1)]\n",
      "cares\n",
      "[('02', 1), ('19', 1)]\n",
      "one\n",
      "[('02', 1), ('04', 1), ('05', 2), ('08', 3), ('09', 1), ('15', 1), ('17', 1), ('18', 1), ('19', 1), ('20', 2)]\n",
      "needs\n",
      "[('02', 1), ('06', 1), ('19', 1)]\n",
      "settled\n",
      "[('02', 1), ('19', 1)]\n",
      "first\n",
      "[('02', 1), ('04', 1), ('05', 1), ('09', 4), ('14', 1), ('15', 3), ('19', 1)]\n",
      "added\n",
      "[('02', 1), ('19', 1)]\n",
      "won\n",
      "[('02', 1), ('05', 1), ('19', 1)]\n",
      "t\n",
      "[('02', 1), ('04', 2), ('05', 2), ('08', 1), ('19', 1)]\n",
      "easy\n",
      "[('02', 1), ('19', 1)]\n",
      "government\n",
      "[('02', 2), ('03', 1), ('19', 2)]\n",
      "deport\n",
      "[('02', 1), ('19', 1)]\n",
      "employees\n",
      "[('02', 1), ('19', 1)]\n",
      "dreamers\n",
      "[('02', 1), ('19', 1)]\n",
      "going\n",
      "[('02', 1), ('08', 1), ('11', 2), ('19', 1)]\n",
      "go\n",
      "[('02', 1), ('19', 1)]\n",
      "us\n",
      "[('02', 1), ('06', 2), ('12', 1), ('19', 1)]\n",
      "get\n",
      "[('02', 1), ('18', 1), ('19', 1)]\n",
      "person\n",
      "[('02', 1), ('19', 1)]\n",
      "latest\n",
      "[('02', 1), ('19', 1)]\n",
      "tech\n",
      "[('02', 1), ('03', 1), ('19', 1)]\n",
      "company\n",
      "[('02', 1), ('08', 1), ('19', 1)]\n",
      "speak\n",
      "[('02', 1), ('19', 1)]\n",
      "made\n",
      "[('02', 1), ('19', 1)]\n",
      "similar\n",
      "[('02', 1), ('19', 1)]\n",
      "comments\n",
      "[('02', 1), ('19', 1)]\n",
      "public\n",
      "[('02', 1), ('16', 1), ('19', 1)]\n",
      "letter\n",
      "[('02', 1), ('19', 1)]\n",
      "putin\n",
      "[('03', 1)]\n",
      "development\n",
      "[('03', 2)]\n",
      "ai\n",
      "[('03', 5)]\n",
      "raises\n",
      "[('03', 1), ('16', 1)]\n",
      "colossal\n",
      "[('03', 1)]\n",
      "opportunities\n",
      "[('03', 1)]\n",
      "threats\n",
      "[('03', 1)]\n",
      "difficult\n",
      "[('03', 1), ('08', 1)]\n",
      "predict\n",
      "[('03', 1)]\n",
      "whoever\n",
      "[('03', 1)]\n",
      "becomes\n",
      "[('03', 1)]\n",
      "leader\n",
      "[('03', 1)]\n",
      "sphere\n",
      "[('03', 1)]\n",
      "become\n",
      "[('03', 2)]\n",
      "ruler\n",
      "[('03', 1)]\n",
      "world\n",
      "[('03', 1)]\n",
      "meeting\n",
      "[('03', 1)]\n",
      "students\n",
      "[('03', 1)]\n",
      "friday\n",
      "[('03', 1), ('07', 1)]\n",
      "united\n",
      "[('03', 1), ('10', 2)]\n",
      "states\n",
      "[('03', 1), ('10', 2)]\n",
      "generally\n",
      "[('03', 1), ('07', 1)]\n",
      "considered\n",
      "[('03', 1)]\n",
      "nation\n",
      "[('03', 1), ('07', 1)]\n",
      "leading\n",
      "[('03', 1)]\n",
      "charge\n",
      "[('03', 1)]\n",
      "towards\n",
      "[('03', 1), ('14', 1)]\n",
      "currently\n",
      "[('03', 1)]\n",
      "giants\n",
      "[('03', 1)]\n",
      "like\n",
      "[('03', 1), ('05', 1)]\n",
      "google\n",
      "[('03', 1)]\n",
      "pouring\n",
      "[('03', 1)]\n",
      "large\n",
      "[('03', 1)]\n",
      "amounts\n",
      "[('03', 1)]\n",
      "cash\n",
      "[('03', 1)]\n",
      "research\n",
      "[('03', 1), ('14', 1), ('16', 1)]\n",
      "projects\n",
      "[('03', 1)]\n",
      "last\n",
      "[('03', 1), ('13', 1), ('16', 1)]\n",
      "week\n",
      "[('03', 1), ('20', 1)]\n",
      "report\n",
      "[('03', 1), ('20', 1)]\n",
      "goldman\n",
      "[('03', 1)]\n",
      "sachs\n",
      "[('03', 1)]\n",
      "found\n",
      "[('03', 1), ('14', 2), ('20', 2)]\n",
      "china\n",
      "[('03', 1), ('13', 1)]\n",
      "capability\n",
      "[('03', 1)]\n",
      "catch\n",
      "[('03', 1)]\n",
      "u\n",
      "[('03', 1), ('05', 1), ('07', 1), ('12', 1), ('13', 2), ('15', 2), ('17', 2)]\n",
      "believe\n",
      "[('03', 1)]\n",
      "technology\n",
      "[('03', 1), ('04', 1)]\n",
      "priority\n",
      "[('03', 1)]\n",
      "agenda\n",
      "[('03', 1)]\n",
      "expect\n",
      "[('03', 1), ('08', 1)]\n",
      "national\n",
      "[('03', 1), ('06', 1), ('17', 1)]\n",
      "regional\n",
      "[('03', 1)]\n",
      "policy\n",
      "[('03', 1)]\n",
      "funding\n",
      "[('03', 1)]\n",
      "support\n",
      "[('03', 1)]\n",
      "follow\n",
      "[('03', 1)]\n",
      "investment\n",
      "[('03', 1), ('12', 2)]\n",
      "bank\n",
      "[('03', 1), ('13', 1), ('20', 2)]\n",
      "meat\n",
      "[('04', 3)]\n",
      "doesn\n",
      "[('04', 1), ('08', 1)]\n",
      "require\n",
      "[('04', 1)]\n",
      "slaughter\n",
      "[('04', 1)]\n",
      "animals\n",
      "[('04', 1)]\n",
      "fashion\n",
      "[('04', 1)]\n",
      "provides\n",
      "[('04', 1)]\n",
      "eco\n",
      "[('04', 1)]\n",
      "friendly\n",
      "[('04', 1)]\n",
      "alternative\n",
      "[('04', 1)]\n",
      "leather\n",
      "[('04', 1)]\n",
      "dairy\n",
      "[('04', 1)]\n",
      "products\n",
      "[('04', 1)]\n",
      "lactose\n",
      "[('04', 1)]\n",
      "free\n",
      "[('04', 1)]\n",
      "brewed\n",
      "[('04', 2)]\n",
      "lab\n",
      "[('04', 3)]\n",
      "san\n",
      "[('04', 1)]\n",
      "franciscans\n",
      "[('04', 1)]\n",
      "aren\n",
      "[('04', 1), ('05', 1)]\n",
      "aware\n",
      "[('04', 1)]\n",
      "heart\n",
      "[('04', 1), ('14', 2)]\n",
      "financial\n",
      "[('04', 1), ('18', 1)]\n",
      "district\n",
      "[('04', 1)]\n",
      "entrepreneurs\n",
      "[('04', 1)]\n",
      "working\n",
      "[('04', 1)]\n",
      "things\n",
      "[('04', 1), ('05', 1)]\n",
      "run\n",
      "[('04', 1), ('08', 1), ('13', 1)]\n",
      "bio\n",
      "[('04', 1)]\n",
      "accelerator\n",
      "[('04', 1)]\n",
      "program\n",
      "[('04', 1), ('10', 1)]\n",
      "called\n",
      "[('04', 1)]\n",
      "indiebio\n",
      "[('04', 2)]\n",
      "funded\n",
      "[('04', 1)]\n",
      "68\n",
      "[('04', 1)]\n",
      "early\n",
      "[('04', 1), ('17', 1)]\n",
      "stage\n",
      "[('04', 1), ('06', 1)]\n",
      "start\n",
      "[('04', 1), ('06', 1)]\n",
      "ups\n",
      "[('04', 1), ('08', 1)]\n",
      "biggest\n",
      "[('04', 1), ('08', 2)]\n",
      "successes\n",
      "[('04', 1)]\n",
      "memphis\n",
      "[('04', 2)]\n",
      "meats\n",
      "[('04', 1)]\n",
      "raised\n",
      "[('04', 1)]\n",
      "17\n",
      "[('04', 1), ('15', 2)]\n",
      "million\n",
      "[('04', 1)]\n",
      "past\n",
      "[('04', 1), ('08', 1)]\n",
      "month\n",
      "[('04', 1), ('13', 1), ('15', 2)]\n",
      "clean\n",
      "[('04', 1)]\n",
      "bioreactor\n",
      "[('04', 1)]\n",
      "slew\n",
      "[('04', 1)]\n",
      "high\n",
      "[('04', 1), ('20', 1)]\n",
      "profile\n",
      "[('04', 1)]\n",
      "investors\n",
      "[('04', 1), ('12', 2), ('18', 1)]\n",
      "including\n",
      "[('04', 1)]\n",
      "billionaires\n",
      "[('04', 1)]\n",
      "gates\n",
      "[('04', 1)]\n",
      "richard\n",
      "[('04', 1)]\n",
      "branson\n",
      "[('04', 1)]\n",
      "elon\n",
      "[('04', 1)]\n",
      "musk\n",
      "[('04', 1)]\n",
      "brother\n",
      "[('04', 1)]\n",
      "kimball\n",
      "[('04', 1)]\n",
      "general\n",
      "[('04', 1), ('05', 1), ('06', 1)]\n",
      "electric\n",
      "[('04', 1)]\n",
      "ceo\n",
      "[('04', 1)]\n",
      "jack\n",
      "[('04', 1)]\n",
      "welch\n",
      "[('04', 1)]\n",
      "investor\n",
      "[('04', 1)]\n",
      "just\n",
      "[('05', 2), ('08', 1), ('15', 1), ('20', 1)]\n",
      "want\n",
      "[('05', 2), ('06', 1)]\n",
      "say\n",
      "[('05', 1), ('07', 1), ('13', 2)]\n",
      "equity\n",
      "[('05', 1)]\n",
      "analogy\n",
      "[('05', 1)]\n",
      "incomplete\n",
      "[('05', 1)]\n",
      "especially\n",
      "[('05', 1)]\n",
      "blockchain\n",
      "[('05', 2)]\n",
      "token\n",
      "[('05', 3)]\n",
      "compliant\n",
      "[('05', 1)]\n",
      "regulations\n",
      "[('05', 1)]\n",
      "need\n",
      "[('05', 1)]\n",
      "utility\n",
      "[('05', 1)]\n",
      "component\n",
      "[('05', 1)]\n",
      "use\n",
      "[('05', 1), ('10', 1), ('11', 1), ('12', 1), ('15', 1), ('16', 1)]\n",
      "proxy\n",
      "[('05', 1)]\n",
      "equities\n",
      "[('05', 3)]\n",
      "hard\n",
      "[('05', 1)]\n",
      "many\n",
      "[('05', 1), ('07', 1), ('12', 1), ('16', 1)]\n",
      "interesting\n",
      "[('05', 1)]\n",
      "can\n",
      "[('05', 2), ('06', 1), ('12', 1), ('13', 1), ('18', 3)]\n",
      "tokenized\n",
      "[('05', 1)]\n",
      "networks\n",
      "[('05', 1)]\n",
      "stickler\n",
      "[('05', 1)]\n",
      "every\n",
      "[('05', 1)]\n",
      "point\n",
      "[('05', 1)]\n",
      "making\n",
      "[('05', 1), ('14', 1)]\n",
      "distinction\n",
      "[('05', 1)]\n",
      "important\n",
      "[('05', 1), ('12', 1), ('20', 1)]\n",
      "keep\n",
      "[('05', 1), ('08', 1)]\n",
      "mind\n",
      "[('05', 1)]\n",
      "work\n",
      "[('05', 1), ('16', 1)]\n",
      "lawyers\n",
      "[('05', 1)]\n",
      "ensure\n",
      "[('05', 1)]\n",
      "tokens\n",
      "[('05', 1)]\n",
      "based\n",
      "[('05', 1), ('07', 1), ('20', 1)]\n",
      "assets\n",
      "[('05', 1), ('07', 1), ('12', 1)]\n",
      "relative\n",
      "[('05', 1)]\n",
      "existing\n",
      "[('05', 1)]\n",
      "secondary\n",
      "[('05', 2)]\n",
      "markets\n",
      "[('05', 2), ('18', 1)]\n",
      "either\n",
      "[('05', 1)]\n",
      "property\n",
      "[('05', 1)]\n",
      "houses\n",
      "[('05', 1)]\n",
      "scale\n",
      "[('05', 1)]\n",
      "happening\n",
      "[('05', 1)]\n",
      "market\n",
      "[('05', 2), ('18', 1)]\n",
      "unparalleled\n",
      "[('05', 1)]\n",
      "thing\n",
      "[('05', 1), ('08', 1)]\n",
      "bunch\n",
      "[('05', 1)]\n",
      "small\n",
      "[('05', 1)]\n",
      "quite\n",
      "[('05', 1)]\n",
      "another\n",
      "[('05', 1)]\n",
      "turn\n",
      "[('05', 1)]\n",
      "internet\n",
      "[('05', 2), ('16', 2)]\n",
      "anyone\n",
      "[('05', 1)]\n",
      "connection\n",
      "[('05', 1)]\n",
      "purchase\n",
      "[('05', 1)]\n",
      "see\n",
      "[('05', 1), ('18', 1)]\n",
      "shapeshift\n",
      "[('05', 1)]\n",
      "io\n",
      "[('05', 1)]\n",
      "great\n",
      "[('05', 1)]\n",
      "example\n",
      "[('05', 1), ('11', 1)]\n",
      "former\n",
      "[('06', 1), ('10', 2)]\n",
      "barack\n",
      "[('06', 1), ('10', 1)]\n",
      "obama\n",
      "[('06', 2), ('10', 3)]\n",
      "decried\n",
      "[('06', 1)]\n",
      "successor\n",
      "[('06', 1)]\n",
      "amnesty\n",
      "[('06', 2)]\n",
      "800\n",
      "[('06', 1), ('10', 1)]\n",
      "000\n",
      "[('06', 1), ('10', 1), ('14', 1), ('15', 1)]\n",
      "people\n",
      "[('06', 2), ('16', 1)]\n",
      "brought\n",
      "[('06', 1), ('10', 2)]\n",
      "america\n",
      "[('06', 1)]\n",
      "illegally\n",
      "[('06', 1)]\n",
      "children\n",
      "[('06', 1), ('10', 1)]\n",
      "describing\n",
      "[('06', 1)]\n",
      "wrong\n",
      "[('06', 3)]\n",
      "self\n",
      "[('06', 2), ('10', 1)]\n",
      "defeating\n",
      "[('06', 2), ('10', 1)]\n",
      "cruel\n",
      "[('06', 2), ('10', 1)]\n",
      "rare\n",
      "[('06', 1)]\n",
      "re\n",
      "[('06', 1), ('10', 1)]\n",
      "entry\n",
      "[('06', 1)]\n",
      "onto\n",
      "[('06', 1)]\n",
      "political\n",
      "[('06', 1), ('10', 1), ('13', 1), ('20', 1)]\n",
      "used\n",
      "[('06', 1), ('11', 2)]\n",
      "facebook\n",
      "[('06', 1), ('10', 1)]\n",
      "post\n",
      "[('06', 1), ('10', 1)]\n",
      "slam\n",
      "[('06', 1), ('15', 2)]\n",
      "call\n",
      "[('06', 1)]\n",
      "target\n",
      "[('06', 1)]\n",
      "young\n",
      "[('06', 1), ('10', 1)]\n",
      "done\n",
      "[('06', 1)]\n",
      "new\n",
      "[('06', 1), ('11', 3), ('20', 2)]\n",
      "businesses\n",
      "[('06', 1)]\n",
      "staff\n",
      "[('06', 1), ('11', 2)]\n",
      "labs\n",
      "[('06', 1)]\n",
      "serve\n",
      "[('06', 1)]\n",
      "military\n",
      "[('06', 1)]\n",
      "otherwise\n",
      "[('06', 1)]\n",
      "contribute\n",
      "[('06', 1)]\n",
      "country\n",
      "[('06', 3), ('10', 2), ('20', 1)]\n",
      "love\n",
      "[('06', 1)]\n",
      "wrote\n",
      "[('06', 1), ('10', 1)]\n",
      "donald\n",
      "[('06', 1)]\n",
      "announcement\n",
      "[('06', 1)]\n",
      "scrap\n",
      "[('06', 1)]\n",
      "programme\n",
      "[('06', 1)]\n",
      "met\n",
      "[('06', 1)]\n",
      "protests\n",
      "[('06', 1)]\n",
      "across\n",
      "[('06', 1), ('14', 1), ('17', 1)]\n",
      "hundreds\n",
      "[('06', 1)]\n",
      "gathered\n",
      "[('06', 1)]\n",
      "outside\n",
      "[('06', 1)]\n",
      "white\n",
      "[('06', 1), ('10', 1)]\n",
      "house\n",
      "[('06', 1), ('10', 1)]\n",
      "protest\n",
      "[('06', 1)]\n",
      "attorney\n",
      "[('06', 1)]\n",
      "jeff\n",
      "[('06', 1)]\n",
      "sessions\n",
      "[('06', 1)]\n",
      "defending\n",
      "[('06', 1)]\n",
      "lawful\n",
      "[('06', 1)]\n",
      "system\n",
      "[('06', 1), ('09', 1), ('20', 1)]\n",
      "immigration\n",
      "[('06', 1)]\n",
      "serves\n",
      "[('06', 1)]\n",
      "interest\n",
      "[('06', 1), ('12', 1)]\n",
      "everyone\n",
      "[('06', 1)]\n",
      "admitted\n",
      "[('06', 1)]\n",
      "bond\n",
      "[('07', 2)]\n",
      "yields\n",
      "[('07', 7)]\n",
      "due\n",
      "[('07', 1)]\n",
      "rise\n",
      "[('07', 1)]\n",
      "big\n",
      "[('07', 1)]\n",
      "way\n",
      "[('07', 1), ('14', 6), ('20', 2)]\n",
      "according\n",
      "[('07', 1), ('08', 1), ('11', 2), ('12', 1), ('13', 1), ('14', 1), ('18', 1)]\n",
      "strategist\n",
      "[('07', 1)]\n",
      "larry\n",
      "[('07', 1)]\n",
      "mcdonald\n",
      "[('07', 3)]\n",
      "acg\n",
      "[('07', 1)]\n",
      "analytics\n",
      "[('07', 1)]\n",
      "year\n",
      "[('07', 1), ('13', 3), ('15', 2)]\n",
      "treasury\n",
      "[('07', 1)]\n",
      "yield\n",
      "[('07', 1)]\n",
      "fell\n",
      "[('07', 1)]\n",
      "2\n",
      "[('07', 1), ('15', 1)]\n",
      "07\n",
      "[('07', 1)]\n",
      "percent\n",
      "[('07', 2), ('08', 1), ('12', 1), ('13', 1), ('20', 1)]\n",
      "lowest\n",
      "[('07', 1)]\n",
      "level\n",
      "[('07', 2), ('13', 1)]\n",
      "since\n",
      "[('07', 2), ('15', 1), ('20', 1)]\n",
      "november\n",
      "[('07', 1)]\n",
      "2016\n",
      "[('07', 1), ('12', 1)]\n",
      "economy\n",
      "[('07', 2)]\n",
      "continues\n",
      "[('07', 1), ('12', 1)]\n",
      "firm\n",
      "[('07', 1), ('08', 1)]\n",
      "footing\n",
      "[('07', 1)]\n",
      "reflects\n",
      "[('07', 1)]\n",
      "global\n",
      "[('07', 1)]\n",
      "jitters\n",
      "[('07', 1)]\n",
      "geopolitical\n",
      "[('07', 1)]\n",
      "risk\n",
      "[('07', 1)]\n",
      "coming\n",
      "[('07', 1), ('14', 1)]\n",
      "north\n",
      "[('07', 1), ('13', 1)]\n",
      "korea\n",
      "[('07', 1), ('13', 1)]\n",
      "driving\n",
      "[('07', 1)]\n",
      "much\n",
      "[('07', 2), ('13', 1), ('16', 1)]\n",
      "lower\n",
      "[('07', 2)]\n",
      "cnbc\n",
      "[('07', 1), ('08', 1)]\n",
      "trading\n",
      "[('07', 1), ('08', 1), ('18', 1)]\n",
      "driven\n",
      "[('07', 1)]\n",
      "unsustainably\n",
      "[('07', 1)]\n",
      "reflect\n",
      "[('07', 1)]\n",
      "strength\n",
      "[('07', 1)]\n",
      "greater\n",
      "[('07', 1)]\n",
      "perceived\n",
      "[('07', 1)]\n",
      "chance\n",
      "[('07', 1)]\n",
      "earning\n",
      "[('07', 1)]\n",
      "money\n",
      "[('07', 1), ('12', 1), ('15', 1), ('18', 5), ('20', 2)]\n",
      "higher\n",
      "[('07', 2)]\n",
      "expected\n",
      "[('07', 1), ('08', 1), ('13', 1), ('17', 1)]\n",
      "rate\n",
      "[('07', 1)]\n",
      "inflation\n",
      "[('07', 1)]\n",
      "loftier\n",
      "[('07', 1)]\n",
      "tend\n",
      "[('07', 1)]\n",
      "metric\n",
      "[('07', 1)]\n",
      "strikingly\n",
      "[('07', 1)]\n",
      "low\n",
      "[('07', 1)]\n",
      "indeed\n",
      "[('07', 1)]\n",
      "points\n",
      "[('07', 1)]\n",
      "reading\n",
      "[('07', 1)]\n",
      "institute\n",
      "[('07', 1)]\n",
      "supply\n",
      "[('07', 1)]\n",
      "management\n",
      "[('07', 1), ('12', 1), ('18', 1)]\n",
      "manufacturing\n",
      "[('07', 1)]\n",
      "index\n",
      "[('07', 1)]\n",
      "came\n",
      "[('07', 1), ('15', 1), ('20', 1)]\n",
      "highest\n",
      "[('07', 1)]\n",
      "2011\n",
      "[('07', 1)]\n",
      "back\n",
      "[('07', 1), ('20', 1)]\n",
      "1\n",
      "[('07', 1), ('14', 1)]\n",
      "5\n",
      "[('07', 1), ('17', 1)]\n",
      "now\n",
      "[('07', 1), ('13', 1), ('18', 1)]\n",
      "noted\n",
      "[('07', 1), ('20', 1)]\n",
      "apple\n",
      "[('08', 3), ('11', 2)]\n",
      "next\n",
      "[('08', 2), ('15', 1)]\n",
      "iphone\n",
      "[('08', 1)]\n",
      "cycle\n",
      "[('08', 2)]\n",
      "yet\n",
      "[('08', 1)]\n",
      "necessarily\n",
      "[('08', 1)]\n",
      "mean\n",
      "[('08', 1)]\n",
      "stock\n",
      "[('08', 1), ('18', 1)]\n",
      "soaring\n",
      "[('08', 1)]\n",
      "well\n",
      "[('08', 1), ('10', 1), ('18', 2)]\n",
      "known\n",
      "[('08', 1), ('18', 1)]\n",
      "followers\n",
      "[('08', 1)]\n",
      "pullback\n",
      "[('08', 1)]\n",
      "shares\n",
      "[('08', 1)]\n",
      "three\n",
      "[('08', 2), ('15', 1)]\n",
      "gene\n",
      "[('08', 1)]\n",
      "munster\n",
      "[('08', 2)]\n",
      "told\n",
      "[('08', 1), ('14', 1)]\n",
      "power\n",
      "[('08', 1)]\n",
      "lunch\n",
      "[('08', 1)]\n",
      "history\n",
      "[('08', 1), ('18', 1)]\n",
      "four\n",
      "[('08', 1), ('09', 1)]\n",
      "years\n",
      "[('08', 2), ('09', 1), ('14', 2), ('15', 1), ('16', 1), ('18', 2)]\n",
      "little\n",
      "[('08', 1)]\n",
      "bit\n",
      "[('08', 1)]\n",
      "look\n",
      "[('08', 1)]\n",
      "happens\n",
      "[('08', 1)]\n",
      "product\n",
      "[('08', 2), ('18', 1)]\n",
      "announced\n",
      "[('08', 1)]\n",
      "covered\n",
      "[('08', 1)]\n",
      "wall\n",
      "[('08', 1), ('18', 2)]\n",
      "street\n",
      "[('08', 1), ('18', 2)]\n",
      "analyst\n",
      "[('08', 1), ('20', 1)]\n",
      "becoming\n",
      "[('08', 1)]\n",
      "managing\n",
      "[('08', 1), ('18', 1)]\n",
      "partner\n",
      "[('08', 1)]\n",
      "venture\n",
      "[('08', 1)]\n",
      "capital\n",
      "[('08', 1), ('13', 1)]\n",
      "loup\n",
      "[('08', 1)]\n",
      "ventures\n",
      "[('08', 1)]\n",
      "suggest\n",
      "[('08', 1), ('20', 1)]\n",
      "typically\n",
      "[('08', 1), ('11', 1)]\n",
      "tail\n",
      "[('08', 1)]\n",
      "launched\n",
      "[('09', 1)]\n",
      "billion\n",
      "[('09', 1), ('20', 1)]\n",
      "mile\n",
      "[('09', 1)]\n",
      "journey\n",
      "[('09', 1)]\n",
      "earth\n",
      "[('09', 1)]\n",
      "oct\n",
      "[('09', 1)]\n",
      "15\n",
      "[('09', 1), ('15', 1), ('20', 1)]\n",
      "1997\n",
      "[('09', 1)]\n",
      "named\n",
      "[('09', 2)]\n",
      "astronomer\n",
      "[('09', 1), ('14', 1)]\n",
      "giovanni\n",
      "[('09', 1)]\n",
      "discovered\n",
      "[('09', 1), ('14', 1)]\n",
      "gap\n",
      "[('09', 1)]\n",
      "carried\n",
      "[('09', 1)]\n",
      "single\n",
      "[('09', 1)]\n",
      "passenger\n",
      "[('09', 1)]\n",
      "huygens\n",
      "[('09', 3)]\n",
      "lander\n",
      "[('09', 1)]\n",
      "built\n",
      "[('09', 1)]\n",
      "european\n",
      "[('09', 1)]\n",
      "space\n",
      "[('09', 1)]\n",
      "agency\n",
      "[('09', 1)]\n",
      "dutch\n",
      "[('09', 1)]\n",
      "scientist\n",
      "[('09', 1)]\n",
      "spotted\n",
      "[('09', 1)]\n",
      "moon\n",
      "[('09', 2)]\n",
      "arrived\n",
      "[('09', 1), ('15', 1)]\n",
      "orbit\n",
      "[('09', 2)]\n",
      "seven\n",
      "[('09', 1), ('15', 1)]\n",
      "launch\n",
      "[('09', 1)]\n",
      "july\n",
      "[('09', 1)]\n",
      "2004\n",
      "[('09', 1), ('15', 1)]\n",
      "several\n",
      "[('09', 1)]\n",
      "later\n",
      "[('09', 1)]\n",
      "split\n",
      "[('09', 1)]\n",
      "touched\n",
      "[('09', 1)]\n",
      "shore\n",
      "[('09', 1)]\n",
      "lakes\n",
      "[('09', 1)]\n",
      "liquid\n",
      "[('09', 1)]\n",
      "methane\n",
      "[('09', 1)]\n",
      "humankind\n",
      "[('09', 1)]\n",
      "ever\n",
      "[('09', 1), ('12', 1)]\n",
      "landing\n",
      "[('09', 2)]\n",
      "kind\n",
      "[('09', 1), ('11', 1)]\n",
      "outer\n",
      "[('09', 1)]\n",
      "solar\n",
      "[('09', 1)]\n",
      "meanwhile\n",
      "[('09', 1)]\n",
      "probe\n",
      "[('09', 1)]\n",
      "pioneer\n",
      "[('09', 1)]\n",
      "voyager\n",
      "[('09', 1)]\n",
      "simply\n",
      "[('09', 1)]\n",
      "flown\n",
      "[('09', 1)]\n",
      "1979\n",
      "[('09', 1)]\n",
      "1980\n",
      "[('09', 1)]\n",
      "respectively\n",
      "[('09', 1)]\n",
      "sharply\n",
      "[('10', 1)]\n",
      "criticized\n",
      "[('10', 1)]\n",
      "protects\n",
      "[('10', 1)]\n",
      "undocumented\n",
      "[('10', 3)]\n",
      "migrants\n",
      "[('10', 1)]\n",
      "deportation\n",
      "[('10', 1)]\n",
      "slamming\n",
      "[('10', 1)]\n",
      "legally\n",
      "[('10', 1)]\n",
      "unnecessary\n",
      "[('10', 1)]\n",
      "page\n",
      "[('10', 1)]\n",
      "defended\n",
      "[('10', 1)]\n",
      "shield\n",
      "[('10', 1)]\n",
      "immigrants\n",
      "[('10', 1)]\n",
      "decided\n",
      "[('10', 1)]\n",
      "established\n",
      "[('10', 1)]\n",
      "principle\n",
      "[('10', 1)]\n",
      "discretion\n",
      "[('10', 1)]\n",
      "protect\n",
      "[('10', 1)]\n",
      "individuals\n",
      "[('10', 1)]\n",
      "know\n",
      "[('10', 3), ('11', 1)]\n",
      "failed\n",
      "[('10', 1)]\n",
      "parents\n",
      "[('10', 1)]\n",
      "sometimes\n",
      "[('10', 1)]\n",
      "even\n",
      "[('10', 2), ('13', 1), ('14', 1), ('18', 1)]\n",
      "infants\n",
      "[('10', 1)]\n",
      "may\n",
      "[('10', 2), ('11', 1), ('20', 1)]\n",
      "besides\n",
      "[('10', 2)]\n",
      "language\n",
      "[('10', 1)]\n",
      "english\n",
      "[('10', 1)]\n",
      "often\n",
      "[('10', 1)]\n",
      "idea\n",
      "[('10', 1)]\n",
      "apply\n",
      "[('10', 1)]\n",
      "job\n",
      "[('10', 1)]\n",
      "college\n",
      "[('10', 1)]\n",
      "driver\n",
      "[('10', 1)]\n",
      "license\n",
      "[('10', 1)]\n",
      "major\n",
      "[('11', 1), ('15', 1), ('16', 1)]\n",
      "league\n",
      "[('11', 1)]\n",
      "baseball\n",
      "[('11', 2)]\n",
      "confirmed\n",
      "[('11', 1), ('14', 1)]\n",
      "boston\n",
      "[('11', 2)]\n",
      "red\n",
      "[('11', 3)]\n",
      "sox\n",
      "[('11', 3)]\n",
      "team\n",
      "[('11', 1)]\n",
      "watch\n",
      "[('11', 2)]\n",
      "steal\n",
      "[('11', 2)]\n",
      "hand\n",
      "[('11', 2)]\n",
      "signals\n",
      "[('11', 2)]\n",
      "york\n",
      "[('11', 3), ('20', 1)]\n",
      "yankees\n",
      "[('11', 2)]\n",
      "times\n",
      "[('11', 2), ('14', 1)]\n",
      "teams\n",
      "[('11', 1)]\n",
      "sorts\n",
      "[('11', 1)]\n",
      "reasons\n",
      "[('11', 1)]\n",
      "tell\n",
      "[('11', 1)]\n",
      "players\n",
      "[('11', 2)]\n",
      "bases\n",
      "[('11', 1)]\n",
      "pitches\n",
      "[('11', 1)]\n",
      "throw\n",
      "[('11', 2)]\n",
      "caught\n",
      "[('11', 1)]\n",
      "member\n",
      "[('11', 2)]\n",
      "training\n",
      "[('11', 1), ('15', 1)]\n",
      "looking\n",
      "[('11', 1)]\n",
      "dugout\n",
      "[('11', 1)]\n",
      "relaying\n",
      "[('11', 1)]\n",
      "message\n",
      "[('11', 1)]\n",
      "able\n",
      "[('11', 1), ('18', 2)]\n",
      "information\n",
      "[('11', 2), ('16', 1)]\n",
      "type\n",
      "[('11', 1)]\n",
      "pitch\n",
      "[('11', 1)]\n",
      "thrown\n",
      "[('11', 1)]\n",
      "pitcher\n",
      "[('11', 1)]\n",
      "fastball\n",
      "[('11', 1)]\n",
      "relayed\n",
      "[('11', 1)]\n",
      "batter\n",
      "[('11', 1)]\n",
      "socially\n",
      "[('12', 1)]\n",
      "responsible\n",
      "[('12', 4)]\n",
      "investing\n",
      "[('12', 1), ('18', 1)]\n",
      "attracting\n",
      "[('12', 1)]\n",
      "dollars\n",
      "[('12', 1)]\n",
      "fact\n",
      "[('12', 1)]\n",
      "following\n",
      "[('12', 1)]\n",
      "recent\n",
      "[('12', 1)]\n",
      "developments\n",
      "[('12', 1)]\n",
      "around\n",
      "[('12', 1), ('14', 1)]\n",
      "paris\n",
      "[('12', 1)]\n",
      "climate\n",
      "[('12', 1)]\n",
      "agreement\n",
      "[('12', 1)]\n",
      "likely\n",
      "[('12', 1), ('14', 1)]\n",
      "asking\n",
      "[('12', 1)]\n",
      "make\n",
      "[('12', 1), ('18', 1)]\n",
      "positive\n",
      "[('12', 1)]\n",
      "impact\n",
      "[('12', 2)]\n",
      "total\n",
      "[('12', 1), ('20', 1)]\n",
      "domiciled\n",
      "[('12', 1)]\n",
      "using\n",
      "[('12', 1), ('14', 1)]\n",
      "sustainable\n",
      "[('12', 3)]\n",
      "strategies\n",
      "[('12', 1)]\n",
      "grew\n",
      "[('12', 1), ('15', 1)]\n",
      "8\n",
      "[('12', 1)]\n",
      "72\n",
      "[('12', 1)]\n",
      "trillion\n",
      "[('12', 2), ('14', 2), ('20', 1)]\n",
      "6\n",
      "[('12', 1), ('13', 2)]\n",
      "57\n",
      "[('12', 1)]\n",
      "2014\n",
      "[('12', 1)]\n",
      "increase\n",
      "[('12', 1)]\n",
      "33\n",
      "[('12', 1)]\n",
      "study\n",
      "[('12', 1), ('16', 1)]\n",
      "sif\n",
      "[('12', 1)]\n",
      "forum\n",
      "[('12', 1)]\n",
      "sri\n",
      "[('12', 1)]\n",
      "grow\n",
      "[('12', 1)]\n",
      "understand\n",
      "[('12', 1), ('14', 1)]\n",
      "evaluate\n",
      "[('12', 1)]\n",
      "different\n",
      "[('12', 1), ('16', 2)]\n",
      "methods\n",
      "[('12', 1)]\n",
      "available\n",
      "[('12', 1)]\n",
      "set\n",
      "[('12', 1)]\n",
      "measurable\n",
      "[('12', 1)]\n",
      "goals\n",
      "[('12', 1)]\n",
      "build\n",
      "[('12', 1)]\n",
      "strategy\n",
      "[('12', 1), ('20', 2)]\n",
      "leaving\n",
      "[('12', 1)]\n",
      "lasting\n",
      "[('12', 1)]\n",
      "legacy\n",
      "[('12', 1)]\n",
      "yuan\n",
      "[('13', 4)]\n",
      "tear\n",
      "[('13', 1)]\n",
      "recouped\n",
      "[('13', 1)]\n",
      "losses\n",
      "[('13', 1)]\n",
      "analysts\n",
      "[('13', 1), ('20', 1)]\n",
      "still\n",
      "[('13', 1), ('18', 1)]\n",
      "room\n",
      "[('13', 1)]\n",
      "strongest\n",
      "[('13', 1)]\n",
      "changing\n",
      "[('13', 1)]\n",
      "hands\n",
      "[('13', 1)]\n",
      "5151\n",
      "[('13', 1)]\n",
      "dollar\n",
      "[('13', 1)]\n",
      "gains\n",
      "[('13', 1)]\n",
      "reuters\n",
      "[('13', 1)]\n",
      "data\n",
      "[('13', 1)]\n",
      "currency\n",
      "[('13', 2), ('20', 1)]\n",
      "appreciation\n",
      "[('13', 1)]\n",
      "quickened\n",
      "[('13', 1)]\n",
      "pace\n",
      "[('13', 1)]\n",
      "august\n",
      "[('13', 1)]\n",
      "marking\n",
      "[('13', 1)]\n",
      "best\n",
      "[('13', 1)]\n",
      "2017\n",
      "[('13', 1)]\n",
      "strengthened\n",
      "[('13', 1)]\n",
      "faster\n",
      "[('13', 1)]\n",
      "bucked\n",
      "[('13', 1)]\n",
      "trend\n",
      "[('13', 1)]\n",
      "asian\n",
      "[('13', 1)]\n",
      "currencies\n",
      "[('13', 1)]\n",
      "falling\n",
      "[('13', 1)]\n",
      "face\n",
      "[('13', 1), ('15', 1)]\n",
      "rising\n",
      "[('13', 1)]\n",
      "tensions\n",
      "[('13', 1)]\n",
      "chinese\n",
      "[('13', 1)]\n",
      "actually\n",
      "[('13', 1)]\n",
      "continued\n",
      "[('13', 1), ('17', 1)]\n",
      "appreciating\n",
      "[('13', 1)]\n",
      "experts\n",
      "[('13', 1)]\n",
      "central\n",
      "[('13', 1), ('20', 1)]\n",
      "succeeded\n",
      "[('13', 1)]\n",
      "demonstrating\n",
      "[('13', 1)]\n",
      "withstand\n",
      "[('13', 1)]\n",
      "downward\n",
      "[('13', 1)]\n",
      "pressure\n",
      "[('13', 1)]\n",
      "tightening\n",
      "[('13', 1)]\n",
      "controls\n",
      "[('13', 1)]\n",
      "foreign\n",
      "[('13', 1)]\n",
      "exchange\n",
      "[('13', 1)]\n",
      "intervention\n",
      "[('13', 1)]\n",
      "huge\n",
      "[('14', 1), ('18', 1)]\n",
      "blackhole\n",
      "[('14', 2)]\n",
      "100\n",
      "[('14', 1)]\n",
      "massive\n",
      "[('14', 2)]\n",
      "lurking\n",
      "[('14', 1)]\n",
      "toxic\n",
      "[('14', 1)]\n",
      "gas\n",
      "[('14', 2)]\n",
      "cloud\n",
      "[('14', 4)]\n",
      "near\n",
      "[('14', 1)]\n",
      "milky\n",
      "[('14', 6)]\n",
      "object\n",
      "[('14', 2)]\n",
      "rank\n",
      "[('14', 1)]\n",
      "second\n",
      "[('14', 1), ('15', 1)]\n",
      "largest\n",
      "[('14', 1)]\n",
      "supermassive\n",
      "[('14', 2)]\n",
      "sagittarius\n",
      "[('14', 2)]\n",
      "located\n",
      "[('14', 1)]\n",
      "centre\n",
      "[('14', 3)]\n",
      "galaxy\n",
      "[('14', 3)]\n",
      "astronomers\n",
      "[('14', 1)]\n",
      "keio\n",
      "[('14', 2)]\n",
      "university\n",
      "[('14', 2)]\n",
      "japan\n",
      "[('14', 1)]\n",
      "alma\n",
      "[('14', 1)]\n",
      "telescope\n",
      "[('14', 1)]\n",
      "chile\n",
      "[('14', 1)]\n",
      "observing\n",
      "[('14', 1)]\n",
      "movement\n",
      "[('14', 1)]\n",
      "gases\n",
      "[('14', 1)]\n",
      "molecules\n",
      "[('14', 1)]\n",
      "elliptical\n",
      "[('14', 1)]\n",
      "200\n",
      "[('14', 1)]\n",
      "light\n",
      "[('14', 1)]\n",
      "150\n",
      "[('14', 1)]\n",
      "kilometres\n",
      "[('14', 2)]\n",
      "wide\n",
      "[('14', 1)]\n",
      "pulled\n",
      "[('14', 1)]\n",
      "immense\n",
      "[('14', 1)]\n",
      "gravitational\n",
      "[('14', 1)]\n",
      "forces\n",
      "[('14', 1)]\n",
      "cause\n",
      "[('14', 1)]\n",
      "computer\n",
      "[('14', 1), ('16', 1)]\n",
      "models\n",
      "[('14', 1)]\n",
      "black\n",
      "[('14', 5), ('20', 1)]\n",
      "hole\n",
      "[('14', 5)]\n",
      "4\n",
      "[('14', 1)]\n",
      "detected\n",
      "[('14', 1)]\n",
      "radio\n",
      "[('14', 1)]\n",
      "waves\n",
      "[('14', 1)]\n",
      "indicated\n",
      "[('14', 1)]\n",
      "presence\n",
      "[('14', 1)]\n",
      "detection\n",
      "[('14', 1)]\n",
      "intermediate\n",
      "[('14', 1)]\n",
      "mass\n",
      "[('14', 1)]\n",
      "candidate\n",
      "[('14', 1)]\n",
      "tomoharu\n",
      "[('14', 1)]\n",
      "oka\n",
      "[('14', 3)]\n",
      "newly\n",
      "[('14', 1)]\n",
      "core\n",
      "[('14', 1)]\n",
      "old\n",
      "[('14', 1), ('15', 2)]\n",
      "dwarf\n",
      "[('14', 1)]\n",
      "cannibalised\n",
      "[('14', 1)]\n",
      "formation\n",
      "[('14', 1)]\n",
      "billions\n",
      "[('14', 1)]\n",
      "ago\n",
      "[('14', 1)]\n",
      "mr\n",
      "[('14', 2)]\n",
      "guardian\n",
      "[('14', 1)]\n",
      "time\n",
      "[('14', 1), ('15', 1), ('18', 1)]\n",
      "drawn\n",
      "[('14', 1)]\n",
      "sink\n",
      "[('14', 1)]\n",
      "published\n",
      "[('14', 1)]\n",
      "journal\n",
      "[('14', 1)]\n",
      "nature\n",
      "[('14', 1)]\n",
      "astronomy\n",
      "[('14', 1)]\n",
      "suspension\n",
      "[('15', 1)]\n",
      "drug\n",
      "[('15', 1)]\n",
      "maria\n",
      "[('15', 1)]\n",
      "sharapova\n",
      "[('15', 3)]\n",
      "returned\n",
      "[('15', 1)]\n",
      "open\n",
      "[('15', 1), ('16', 1)]\n",
      "monday\n",
      "[('15', 1)]\n",
      "night\n",
      "[('15', 1), ('17', 1)]\n",
      "30\n",
      "[('15', 1), ('18', 1)]\n",
      "memorable\n",
      "[('15', 1)]\n",
      "performance\n",
      "[('15', 1)]\n",
      "lights\n",
      "[('15', 1)]\n",
      "arthur\n",
      "[('15', 1)]\n",
      "ashe\n",
      "[('15', 1)]\n",
      "stadium\n",
      "[('15', 1)]\n",
      "downing\n",
      "[('15', 1)]\n",
      "seed\n",
      "[('15', 1)]\n",
      "simona\n",
      "[('15', 1)]\n",
      "halep\n",
      "[('15', 1)]\n",
      "today\n",
      "[('15', 1)]\n",
      "ll\n",
      "[('15', 1), ('18', 1)]\n",
      "timea\n",
      "[('15', 1)]\n",
      "babos\n",
      "[('15', 1)]\n",
      "round\n",
      "[('15', 1)]\n",
      "decade\n",
      "[('15', 1)]\n",
      "clinched\n",
      "[('15', 1)]\n",
      "grand\n",
      "[('15', 2)]\n",
      "championship\n",
      "[('15', 1)]\n",
      "age\n",
      "[('15', 2)]\n",
      "defeated\n",
      "[('15', 1)]\n",
      "serena\n",
      "[('15', 1)]\n",
      "williams\n",
      "[('15', 1)]\n",
      "wimbledon\n",
      "[('15', 1)]\n",
      "five\n",
      "[('15', 1)]\n",
      "titles\n",
      "[('15', 1)]\n",
      "sizable\n",
      "[('15', 1)]\n",
      "paycheck\n",
      "[('15', 1)]\n",
      "560\n",
      "[('15', 1)]\n",
      "500\n",
      "[('15', 1)]\n",
      "724\n",
      "[('15', 1)]\n",
      "lot\n",
      "[('15', 1)]\n",
      "teenager\n",
      "[('15', 1)]\n",
      "particularly\n",
      "[('15', 1), ('16', 1)]\n",
      "poor\n",
      "[('15', 1)]\n",
      "russian\n",
      "[('15', 1)]\n",
      "born\n",
      "[('15', 1)]\n",
      "athlete\n",
      "[('15', 1)]\n",
      "dad\n",
      "[('15', 2)]\n",
      "700\n",
      "[('15', 1)]\n",
      "nick\n",
      "[('15', 1)]\n",
      "bollettieri\n",
      "[('15', 1)]\n",
      "tennis\n",
      "[('15', 1)]\n",
      "academy\n",
      "[('15', 1)]\n",
      "slept\n",
      "[('15', 1)]\n",
      "pullout\n",
      "[('15', 1)]\n",
      "couch\n",
      "[('15', 1)]\n",
      "250\n",
      "[('15', 1)]\n",
      "apartment\n",
      "[('15', 1)]\n",
      "secured\n",
      "[('15', 1)]\n",
      "figure\n",
      "[('15', 1)]\n",
      "check\n",
      "[('15', 1)]\n",
      "headed\n",
      "[('15', 1)]\n",
      "straight\n",
      "[('15', 1)]\n",
      "tj\n",
      "[('15', 1)]\n",
      "maxx\n",
      "[('15', 1)]\n",
      "1970s\n",
      "[('16', 1)]\n",
      "1980s\n",
      "[('16', 1)]\n",
      "social\n",
      "[('16', 2)]\n",
      "informat\n",
      "[('16', 1)]\n",
      "ics\n",
      "[('16', 1)]\n",
      "focused\n",
      "[('16', 1), ('20', 1)]\n",
      "organizations\n",
      "[('16', 1)]\n",
      "sites\n",
      "[('16', 1)]\n",
      "computerization\n",
      "[('16', 1)]\n",
      "technical\n",
      "[('16', 1)]\n",
      "specialists\n",
      "[('16', 1)]\n",
      "gotten\n",
      "[('16', 1)]\n",
      "systems\n",
      "[('16', 1)]\n",
      "access\n",
      "[('16', 2), ('18', 1)]\n",
      "issues\n",
      "[('16', 1)]\n",
      "communication\n",
      "[('16', 1)]\n",
      "entertainment\n",
      "[('16', 1)]\n",
      "medical\n",
      "[('16', 1)]\n",
      "personal\n",
      "[('16', 1)]\n",
      "uses\n",
      "[('16', 1)]\n",
      "significant\n",
      "[('16', 1)]\n",
      "phenomena\n",
      "[('16', 1)]\n",
      "topics\n",
      "[('16', 1)]\n",
      "emphasize\n",
      "[('16', 1)]\n",
      "part\n",
      "[('16', 1)]\n",
      "informatics\n",
      "[('16', 1)]\n",
      "lines\n",
      "[('16', 1)]\n",
      "analysis\n",
      "[('16', 1), ('18', 1)]\n",
      "warrant\n",
      "[('16', 1)]\n",
      "serious\n",
      "[('16', 1)]\n",
      "hurricane\n",
      "[('17', 3)]\n",
      "irma\n",
      "[('17', 3)]\n",
      "forceful\n",
      "[('17', 1)]\n",
      "atlantic\n",
      "[('17', 1)]\n",
      "storms\n",
      "[('17', 1)]\n",
      "century\n",
      "[('17', 1)]\n",
      "churned\n",
      "[('17', 1)]\n",
      "ocean\n",
      "[('17', 1)]\n",
      "collision\n",
      "[('17', 1)]\n",
      "course\n",
      "[('17', 1)]\n",
      "puerto\n",
      "[('17', 2)]\n",
      "rico\n",
      "[('17', 2)]\n",
      "virgin\n",
      "[('17', 1)]\n",
      "islands\n",
      "[('17', 2)]\n",
      "bearing\n",
      "[('17', 1)]\n",
      "northern\n",
      "[('17', 2)]\n",
      "caribbean\n",
      "[('17', 1)]\n",
      "devastating\n",
      "[('17', 1)]\n",
      "mix\n",
      "[('17', 1)]\n",
      "fierce\n",
      "[('17', 1)]\n",
      "winds\n",
      "[('17', 2)]\n",
      "surf\n",
      "[('17', 1)]\n",
      "rain\n",
      "[('17', 1)]\n",
      "eye\n",
      "[('17', 1)]\n",
      "category\n",
      "[('17', 1)]\n",
      "storm\n",
      "[('17', 2)]\n",
      "packing\n",
      "[('17', 1)]\n",
      "185\n",
      "[('17', 1)]\n",
      "miles\n",
      "[('17', 1)]\n",
      "per\n",
      "[('17', 2)]\n",
      "hour\n",
      "[('17', 2)]\n",
      "295\n",
      "[('17', 1)]\n",
      "km\n",
      "[('17', 1)]\n",
      "sweep\n",
      "[('17', 1)]\n",
      "leeward\n",
      "[('17', 1)]\n",
      "east\n",
      "[('17', 1)]\n",
      "wednesday\n",
      "[('17', 1)]\n",
      "en\n",
      "[('17', 1)]\n",
      "route\n",
      "[('17', 1)]\n",
      "florida\n",
      "[('17', 1)]\n",
      "landfall\n",
      "[('17', 1)]\n",
      "saturday\n",
      "[('17', 1)]\n",
      "center\n",
      "[('17', 1)]\n",
      "nhc\n",
      "[('17', 2)]\n",
      "miami\n",
      "[('17', 1)]\n",
      "reported\n",
      "[('17', 1)]\n",
      "threat\n",
      "[('17', 1)]\n",
      "posed\n",
      "[('17', 1)]\n",
      "mainland\n",
      "[('17', 1)]\n",
      "described\n",
      "[('17', 1)]\n",
      "forecasters\n",
      "[('17', 1)]\n",
      "potentially\n",
      "[('17', 1)]\n",
      "catastrophic\n",
      "[('17', 1)]\n",
      "loomed\n",
      "[('17', 1)]\n",
      "texas\n",
      "[('17', 1)]\n",
      "louisiana\n",
      "[('17', 1)]\n",
      "reel\n",
      "[('17', 1)]\n",
      "widespread\n",
      "[('17', 1)]\n",
      "destructive\n",
      "[('17', 1)]\n",
      "flooding\n",
      "[('17', 1)]\n",
      "harvey\n",
      "[('17', 1)]\n",
      "jim\n",
      "[('18', 1)]\n",
      "cramer\n",
      "[('18', 7)]\n",
      "always\n",
      "[('18', 1)]\n",
      "bull\n",
      "[('18', 1)]\n",
      "somewhere\n",
      "[('18', 1)]\n",
      "toughest\n",
      "[('18', 1)]\n",
      "find\n",
      "[('18', 1)]\n",
      "pockets\n",
      "[('18', 1)]\n",
      "opportunity\n",
      "[('18', 1)]\n",
      "became\n",
      "[('18', 1)]\n",
      "successful\n",
      "[('18', 1)]\n",
      "managers\n",
      "[('18', 1)]\n",
      "14\n",
      "[('18', 1)]\n",
      "hedge\n",
      "[('18', 1)]\n",
      "fund\n",
      "[('18', 1)]\n",
      "berkowitz\n",
      "[('18', 1)]\n",
      "generated\n",
      "[('18', 1)]\n",
      "average\n",
      "[('18', 1)]\n",
      "annual\n",
      "[('18', 1), ('20', 1)]\n",
      "return\n",
      "[('18', 1)]\n",
      "24\n",
      "[('18', 1)]\n",
      "individual\n",
      "[('18', 1)]\n",
      "success\n",
      "[('18', 1), ('20', 1)]\n",
      "holding\n",
      "[('18', 1)]\n",
      "secrets\n",
      "[('18', 1)]\n",
      "rather\n",
      "[('18', 1)]\n",
      "limiting\n",
      "[('18', 1)]\n",
      "skills\n",
      "[('18', 1)]\n",
      "benefit\n",
      "[('18', 2)]\n",
      "granting\n",
      "[('18', 1)]\n",
      "subscriber\n",
      "[('18', 1)]\n",
      "portfolio\n",
      "[('18', 1)]\n",
      "membership\n",
      "[('18', 1)]\n",
      "club\n",
      "[('18', 1)]\n",
      "action\n",
      "[('18', 1)]\n",
      "alerts\n",
      "[('18', 2)]\n",
      "plus\n",
      "[('18', 1)]\n",
      "buy\n",
      "[('18', 1)]\n",
      "price\n",
      "[('18', 2)]\n",
      "current\n",
      "[('18', 1)]\n",
      "returns\n",
      "[('18', 1)]\n",
      "holdings\n",
      "[('18', 1)]\n",
      "immediately\n",
      "[('18', 1)]\n",
      "upon\n",
      "[('18', 1)]\n",
      "signup\n",
      "[('18', 1)]\n",
      "behind\n",
      "[('18', 1)]\n",
      "decisions\n",
      "[('18', 1)]\n",
      "arguably\n",
      "[('18', 1)]\n",
      "valuable\n",
      "[('18', 1)]\n",
      "offerings\n",
      "[('18', 1)]\n",
      "trade\n",
      "[('18', 2)]\n",
      "emails\n",
      "[('18', 1)]\n",
      "right\n",
      "[('18', 1)]\n",
      "makes\n",
      "[('18', 1)]\n",
      "allows\n",
      "[('18', 1)]\n",
      "invest\n",
      "[('18', 2)]\n",
      "likewell\n",
      "[('18', 1)]\n",
      "isgetting\n",
      "[('18', 1)]\n",
      "skilled\n",
      "[('18', 1)]\n",
      "manager\n",
      "[('18', 2)]\n",
      "insight\n",
      "[('18', 1)]\n",
      "maintaining\n",
      "[('18', 1)]\n",
      "control\n",
      "[('18', 1)]\n",
      "please\n",
      "[('18', 1)]\n",
      "walks\n",
      "[('18', 1)]\n",
      "everything\n",
      "[('18', 1)]\n",
      "moves\n",
      "[('18', 1)]\n",
      "professional\n",
      "[('18', 1)]\n",
      "without\n",
      "[('18', 1)]\n",
      "spending\n",
      "[('18', 1)]\n",
      "india\n",
      "[('20', 2)]\n",
      "demonetized\n",
      "[('20', 1)]\n",
      "far\n",
      "[('20', 1)]\n",
      "tarnishing\n",
      "[('20', 1)]\n",
      "prime\n",
      "[('20', 2)]\n",
      "minister\n",
      "[('20', 2)]\n",
      "narendra\n",
      "[('20', 2)]\n",
      "modi\n",
      "[('20', 2)]\n",
      "image\n",
      "[('20', 1)]\n",
      "ultimately\n",
      "[('20', 1)]\n",
      "viewed\n",
      "[('20', 1)]\n",
      "remember\n",
      "[('20', 1)]\n",
      "demonetization\n",
      "[('20', 2)]\n",
      "intended\n",
      "[('20', 1)]\n",
      "economic\n",
      "[('20', 1)]\n",
      "south\n",
      "[('20', 1)]\n",
      "asia\n",
      "[('20', 1)]\n",
      "eurasia\n",
      "[('20', 1)]\n",
      "group\n",
      "[('20', 1)]\n",
      "sasha\n",
      "[('20', 1)]\n",
      "riser\n",
      "[('20', 1)]\n",
      "kositsky\n",
      "[('20', 1)]\n",
      "drama\n",
      "[('20', 1)]\n",
      "allowed\n",
      "[('20', 1)]\n",
      "demonstrate\n",
      "[('20', 1)]\n",
      "visible\n",
      "[('20', 1)]\n",
      "commitment\n",
      "[('20', 1)]\n",
      "fighting\n",
      "[('20', 1)]\n",
      "corruption\n",
      "[('20', 1)]\n",
      "earlier\n",
      "[('20', 1)]\n",
      "reserve\n",
      "[('20', 1)]\n",
      "rbi\n",
      "[('20', 1)]\n",
      "28\n",
      "[('20', 1)]\n",
      "rupees\n",
      "[('20', 1)]\n",
      "239\n",
      "[('20', 1)]\n",
      "worth\n",
      "[('20', 1)]\n",
      "cancelled\n",
      "[('20', 1)]\n",
      "value\n",
      "[('20', 1)]\n",
      "notes\n",
      "[('20', 1)]\n",
      "deposited\n",
      "[('20', 1)]\n",
      "exchanged\n",
      "[('20', 1)]\n",
      "implemented\n",
      "[('20', 1)]\n",
      "shy\n",
      "[('20', 1)]\n",
      "number\n",
      "[('20', 1)]\n",
      "circulation\n",
      "[('20', 1)]\n",
      "plans\n",
      "[('20', 1)]\n"
     ]
    }
   ],
   "source": [
    "word_index = {}  # We are creating an empty dictionary to store the word frequency index.\n",
    "\n",
    "# Lambda to increment word frequency in the word index\n",
    "increment_word_frequency = lambda word, file_name: word_index.setdefault(word, {}).setdefault(file_name, 0)\n",
    "for file_name, words in final_words.items():\n",
    "    for word in words:\n",
    "        increment_word_frequency(word, file_name)\n",
    "        word_index[word][file_name] += 1  \n",
    "for word, doc_freqs in word_index.items():\n",
    "    # Create a list of tuples, where each tuple contains the document name (without extension) and the word frequency\n",
    "    doc_list = list(map(lambda doc_freq: (doc_freq[0].split('.')[0], doc_freq[1]), doc_freqs.items()))\n",
    "    print(f\"{word}\\n{doc_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fire a Query with lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a set of words as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query words: ['my', 'name', 'is', 'swaraj,', 'i', 'am', 'from', 'nasa,', 'i', 'love', 'planet', 'krishna,', 'plant', 'circulation', 'burn', 'days']\n"
     ]
    }
   ],
   "source": [
    "# Lambda function to convert a string to lowercase and split it into words\n",
    "process_query = lambda q: q.lower().split()\n",
    "\n",
    "# Example\n",
    "query = \"My name is Swaraj, I am from Nasa, I love planet Krishna, plant circulation burn days\"\n",
    "query_words = process_query(query)\n",
    "print(f\"Query words: {query_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a list with words from each text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'my' is not found in the index.\n",
      "'name' is not found in the index.\n",
      "'is' is not found in the index.\n",
      "'swaraj,' is not found in the index.\n",
      "'i' is not found in the index.\n",
      "'am' is not found in the index.\n",
      "'from' is not found in the index.\n",
      "'nasa,' is not found in the index.\n",
      "'i' is not found in the index.\n",
      "'love' is found in the index.\n",
      "'planet' is found in the index.\n",
      "'krishna,' is not found in the index.\n",
      "'plant' is not found in the index.\n",
      "'circulation' is found in the index.\n",
      "'burn' is found in the index.\n",
      "'days' is found in the index.\n"
     ]
    }
   ],
   "source": [
    "# Lambda function to check if a word is in the word_index\n",
    "check_word_in_index = lambda word: f\"'{word}' is found in the index.\" if word in word_index else f\"'{word}' is not found in the index.\"\n",
    "for word in query_words:\n",
    "    print(check_word_in_index(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered query words (after removing stopwords): ['name', 'swaraj,', 'nasa,', 'love', 'planet', 'krishna,', 'plant', 'circulation', 'burn', 'days']\n"
     ]
    }
   ],
   "source": [
    "# Lambda function to load stopwords from a file\n",
    "load_stopwords = lambda filepath: set(open(filepath, 'r').read().split())\n",
    "stopwords = load_stopwords('/Volumes/Jagannath/Fall 2024/Cloud Computing/Assignment_1_data/stopwords.txt')\n",
    "# Lambda function to filter out stopwords from the query\n",
    "filter_query = lambda words, stopwords: [word for word in words if word not in stopwords]\n",
    "filtered_query = filter_query(query_words, stopwords)\n",
    "print(f\"Filtered query words (after removing stopwords): {filtered_query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score each document by summing the frequency of each word of the input in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document scores before sorting:\n",
      "06.txt.txt: 1\n",
      "01.txt.txt: 4\n",
      "09.txt.txt: 1\n",
      "20.txt.txt: 1\n"
     ]
    }
   ],
   "source": [
    "doc_scores = {}\n",
    "update_doc_scores = lambda doc, freq: doc_scores.update({doc: doc_scores.get(doc, 0) + freq})\n",
    "\n",
    "for word in filtered_query: # # Loop through each word in the filtered query and update the document scores\n",
    "    if word in word_index:\n",
    "        for doc, freq in word_index[word].items():\n",
    "            update_doc_scores(doc, freq)\n",
    "\n",
    "print(\"Document scores before sorting:\")\n",
    "for doc, score in doc_scores.items():\n",
    "    print(f\"{doc}: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print pair wise all document ID and score in descending order of score whose score is greater than zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Documents and their scores by total frequency:\n",
      "01.txt.txt: 4\n",
      "06.txt.txt: 1\n",
      "09.txt.txt: 1\n",
      "20.txt.txt: 1\n"
     ]
    }
   ],
   "source": [
    "# Lambda function to filter documents with scores greater than 0\n",
    "filter_docs = lambda doc_scores: [(doc, score) for doc, score in doc_scores.items() if score > 0]\n",
    "\n",
    "# Lambda function to sort the documents by score in descending order\n",
    "sort_docs = lambda docs: sorted(docs, key=lambda x: x[1], reverse=True)\n",
    "sorted_docs = sort_docs(filter_docs(doc_scores))\n",
    "\n",
    "# Result\n",
    "print(\"\\nDocuments and their scores by total frequency:\")\n",
    "for doc, score in sorted_docs:\n",
    "    print(f\"{doc}: {score}\")\n",
    "if not sorted_docs:\n",
    "    print(\"No documents matched.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
